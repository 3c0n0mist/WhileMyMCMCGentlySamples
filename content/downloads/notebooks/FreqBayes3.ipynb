{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequentism and Bayesianism III: Confidence vs. Credibility"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 1: Finding the Mean"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by again examining an extremely simple problem; this is the one we started with in the [first post](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) of this series: finding the mean of a Gaussian distribution. Previously we just looked at the maximum likelihood and maximum a posteriori estimates; here we'll extend this and look at credibility intervals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Our Simple Problem: How bright is that star?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imagine you're observing a star that you (somehow) know has a constant brightness. Simplistically, we can think of this brightness as the number of photons reaching our telescope in one second.  Any given measurement of this number is subject to errors: the source of those errors is not important right now, but let's assume the observations $x_i$ are drawn from a normal distribution about the true brightness value $B$ with a known standard deviation $\\sigma_x$.\n",
      "\n",
      "Given a series of measurements, what are the 95% (i.e. $2\\sigma$) limits that we would place on the brightness?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. The Frequentist Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The frequentist approach to this problem is well-understood, and is as follows:\n",
      "\n",
      "For any set of $N$ values $D = \\{x_i\\}_{i=1}^N$, an unbiased estimate of the mean $\\mu$ of the distribution is given by\n",
      "\n",
      "$$\n",
      "\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i\n",
      "$$\n",
      "\n",
      "The **sampling distribution** describes the observed frequency of the estimate of the mean; by the central limit theorem we can show that the sampling distribution is normal; i.e.\n",
      "\n",
      "$$\n",
      "f(\\mu~|~D) = \\mathcal{N}(\\bar{x}, \\sigma_x/\\sqrt{N})\n",
      "$$\n",
      "\n",
      "where we've used the **standard error of the mean**,\n",
      "\n",
      "$$\n",
      "\\sigma_\\mu = \\sigma_x / \\sqrt{N}\n",
      "$$\n",
      "\n",
      "The central limit theorem tells us that this is correct for any generating distribution if $N$ is large; if our generating distribution happens to be Gaussian, it also holds for $N$ as small as 2.\n",
      "\n",
      "Let's quickly check this empirically, by looking at $10^6$ samples of the mean of 5 numbers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "N = 5\n",
      "Nsamp = 10 ** 6\n",
      "x = np.random.normal(0, 2, size=(Nsamp, N))\n",
      "mu_samp = x.mean(1)\n",
      "sig_samp = 2 * N ** -0.5\n",
      "\n",
      "print(\"{0:.3f} should equal {1:.3f}\".format(np.std(mu_samp), sig_samp))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.894 should equal 0.894\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The standard deviation of the observed means is equal to $\\sigma_x N^{-1/2}$, as expected.\n",
      "\n",
      "From this normal sampling distribution, we can quickly write the 95% credibility interval by recalling that two standard deviations is equivalent to 95% of the area under the curve. So our confidence interval is\n",
      "\n",
      "$$\n",
      "CI_{\\mu} =  \\left(\\bar{x} - 2\\sigma_\\mu,~\\bar{x} + 2\\sigma_\\mu\\right)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can try this with a quick example: say we have a star with a true brightness of 100, and we have three observations with error equivalent to a standard deviation of 10. What is our 95% confidence interval on the mean?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(1)\n",
      "D = np.random.normal(100, 10, size=3)\n",
      "D.sort() # for convenience later\n",
      "print(D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  93.88243586   94.71828248  116.24345364]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import erfinv\n",
      "\n",
      "def freq_CI_mu(D, sigma, frac=0.95):\n",
      "    \"\"\"Compute the confidence interval on the mean\"\"\"\n",
      "    Nsigma = np.sqrt(2) * erfinv(frac)\n",
      "    mu = D.mean()\n",
      "    sigma_mu = sigma * D.size ** -0.5\n",
      "    return mu - Nsigma * sigma_mu, mu + Nsigma * sigma_mu\n",
      "\n",
      "print(\"95% Confidence Interval: [{0:.0f}, {1:.0f}]\".format(*freq_CI_mu(D, 10)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "95% Confidence Interval: [90, 113]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note here that we've assumed $\\sigma_x$ is a known quantity; this could also be estimated from the data along with $\\mu$, but we'll keep things simple for sake of example."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. The Bayesian Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the Bayesian approach, we start with Bayes' theorem:\n",
      "\n",
      "$$\n",
      "P(\\mu~|~D) = \\frac{P(D~|~\\mu)P(\\mu)}{P(D)}\n",
      "$$\n",
      "\n",
      "We'll use a flat prior on $\\mu$ (i.e. $P(\\mu) \\propto 1$ over the region of interest) and use the likelihood\n",
      "\n",
      "$$\n",
      "P(D~|~\\mu) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}}\\exp\\left[\\frac{(\\mu - x_i)^2}{2\\sigma_x^2}\\right]\n",
      "$$\n",
      "\n",
      "Computing this product and manipulating the terms, it's straightforward to show that this gives\n",
      "\n",
      "$$\n",
      "P(\\mu~|~D) \\propto \\exp\\left[\\frac{-(\\mu - \\bar{x})^2}{2\\sigma_\\mu^2}\\right]\n",
      "$$\n",
      "\n",
      "which is recognizable as a normal distribution with mean $\\bar{x}$ and standard deviation $\\sigma_\\mu$.\n",
      "That is, **the Bayesian posterior on $\\mu$ in this case is exactly equal to the frequentist sampling distribution for $\\mu$**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this posterior, we can compute the Bayesian credible region, which looks exactly like the frequentist confidence interval:\n",
      "\n",
      "$$\n",
      "CR_{\\mu} =  \\left(\\bar{x} - 2\\sigma_\\mu,~\\bar{x} + 2\\sigma_\\mu\\right)\n",
      "$$\n",
      "\n",
      "The difference is that here we're assuming $\\sigma_x$ is known; we could easily do the \n",
      "\n",
      "For completeness, we will do the same as above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bayes_CR_mu(D, sigma, frac=0.95):\n",
      "    \"\"\"Compute the credible region on the mean\"\"\"\n",
      "    Nsigma = np.sqrt(2) * erfinv(frac)\n",
      "    mu = D.mean()\n",
      "    sigma_mu = sigma * D.size ** -0.5\n",
      "    return mu - Nsigma * sigma_mu, mu + Nsigma * sigma_mu\n",
      "\n",
      "print(\"95% Credible Region: [{0:.0f}, {1:.0f}]\".format(*bayes_CR_mu(D, 10)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "95% Credible Region: [90, 113]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "So What's the Difference?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above derivation is one reason why the frequentist confidence interval and the Bayesian credible region are so often confused. In many simple problems, they correspond exactly. But we must be clear that **even though the two are numerically equivalent, their interpretation is very different**.\n",
      "\n",
      "Recall that in Bayesianism, the probability distributions reflect our degree of belief. So when I write the above credible region, it's equivalent to saying\n",
      "\n",
      "> \"Given our observed data, there is a 95% probability that the true value of $\\mu$ falls within $CR_\\mu$\" - Bayesians\n",
      "\n",
      "In frequentism, on the other hand, $\\mu$ is considered a fixed value and the data (and all quantities derived from the data) are random variables. So the frequentist confidence interval is equivalent to saying\n",
      "\n",
      "> \"There is a 95% probability that when I compute $CI_\\mu$ from data of this sort, the true mean will fall within $CI_\\mu$.\" - Frequentists\n",
      "\n",
      "The difference is subtle, but it has drastic consequences."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below I'll spend more time describing in words what this difference entails, but here I want to run some simulations to convince ourselves that the interpretation is correct."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Confirming the Bayesian Credible Region"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To confirm what the Bayesian credible region is claiming, we must sample random $\\mu$ values from the prior, sample random sets of points from each $\\mu$, select the sets of points which match our observed data, and then ask how many of these $\\mu$ values are within the credible region we've constructed.\n",
      "\n",
      "In code, that looks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first define some quantities that we need \n",
      "Nsamples = 2E7\n",
      "N = len(D)\n",
      "sigma_x = 10\n",
      "\n",
      "if N * Nsamples > 1E8:\n",
      "    # if someone changes D, this could easily cause a memory error\n",
      "    raise ValueError(\"Are you sure you want this many samples?\")\n",
      "    \n",
      "# eps tells us how close to D we need to be to consider\n",
      "# it a matching sample. The value is a tradeoff between\n",
      "# accuracy of the values and total number of good samples\n",
      "eps = 1\n",
      "np.random.seed(0)\n",
      "\n",
      "# Generate some mean values from the (flat) prior\n",
      "mu = 80 + 40 * np.random.random(Nsamples)\n",
      "\n",
      "# Generate data from the mean values\n",
      "x = np.random.normal(mu, sigma_x, (N, Nsamples)).T\n",
      "x.sort(1)\n",
      "D.sort()\n",
      "\n",
      "# find out which of the generated data matches our true data\n",
      "i = np.all(abs(x - D) < eps, 1)\n",
      "print(\"number of suitable samples: {0}\".format(i.sum()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of suitable samples: 3620\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we ask how many of these mu values fall in our credible region\n",
      "mu_good = mu[i]\n",
      "CR = bayes_CR_mu(D, 10)\n",
      "within_CR = (CR[0] < mu_good) & (mu_good < CR[1])\n",
      "print \"Fraction in Credible Region: {0:.3f}\".format(within_CR.sum() * 1. / within_CR.size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fraction in Credible Region: 0.949\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that, as predicted, the fraction of true $\\mu$ values that lies within the Credible Region is near 95%.\n",
      "\n",
      "The important thing to note here is which of the variables is random, and which are fixed. We compute **a single credible region from our observed data**, and we consider it in terms of **multiple random draws of $\\mu$**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Confirming the frequentist Confidence Interval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To confirm what the frequentist confidence interval is telling us is a bit less involved. What we want to do is to draw sets of values from the distribution defined by $\\mu$, and from each of these values, compute a new credible region. Then we ask what fraction of these credible regions contain the true $\\mu$.\n",
      "\n",
      "In code, it looks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define some quantities we need\n",
      "N = len(D)\n",
      "Nsamples = 1E4\n",
      "mu = 100\n",
      "sigma_x = 10\n",
      "\n",
      "# Draw datasets from the true distribution\n",
      "np.random.seed(0)\n",
      "x = np.random.normal(mu, sigma_x, (Nsamples, N))\n",
      "\n",
      "# Compute a confidence interval from each dataset\n",
      "CIs = np.array([freq_CI_mu(Di, sigma_x) for Di in x])\n",
      "\n",
      "# find which confidence intervals contain the mean\n",
      "contains_mu = (CIs[:, 0] < mu) & (mu < CIs[:, 1])\n",
      "print \"Fraction in Confidence Interval: {0:.3f}\".format(contains_mu.sum() * 1. / contains_mu.size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fraction in Confidence Interval: 0.951\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that, as predicted, 95% of the confidence intervals contain the true value of $\\mu$.\n",
      "\n",
      "Again, the important thing to note here is which of the variables is random. We use **a single value of $\\mu$**, and consider it in relation to **multiple credible regions constructed from random data**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "In Which Jake Pisses Off The Internet..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's think some more about this...\n",
      "\n",
      "Comparing the frequentist confirmation and the Bayesian confirmation above, we see some distinctions which stem from the very [definition of probability](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) that I wrote about in the first post of this series.\n",
      "\n",
      "- Bayesianism treats parameters (e.g. $\\mu$) as random variables, while frequentism treats them as fixed.\n",
      "- Bayesianism treats observed data (e.g. $D$) as fixed, while frequentism treats them as random variables.\n",
      "- Bayesianism treats parameter constraints (e.g. $CR_\\mu$, $CI_\\mu$) as fixed, while frequentism treats them as random variables.\n",
      "\n",
      "And this is where the comparison starts to get meaningful, and where my post starts to get more controversial.\n",
      "\n",
      "Recall the statements about confidence intervals and credible regions that I made above. From the Bayesians:\n",
      "\n",
      "> \"Given our observed data, there is a 95% probability that the true value of $\\mu$ falls within $CR_\\mu$\" - Bayesians\n",
      "\n",
      "And from the frequentists:\n",
      "\n",
      "> \"There is a 95% probability that when I compute $CI_\\mu$ from data of this sort, the true mean will fall within $CI_\\mu$.\" - Frequentists\n",
      "\n",
      "Now think about what this means. Suppose you're a scientist who has taken three observations of the brightness of a star. What you care about are **those three observations**.  In other words, you care about what you can say \"Given the observed data\", not what might happen when you have hypothetical \"data of this sort\".\n",
      "\n",
      "But suppose, now, that you ask the frequentist confidence interval what it can tell you given *this particular data that I've observed*. Here's what it has to say:\n",
      "\n",
      "> \"Given this observed data, the true value of $\\mu$ is either in our confidence interval or it isn't\" - Frequentists\n",
      "\n",
      "That's all the credibility region means for **this particular data** that you have observed. The astute reader will notice that this is nothing but a tautology. We can put this more succinctly:\n",
      "\n",
      "> \"Given this observed data, I can put no constraint on the value of $\\mu$\" - Frequentists\n",
      "\n",
      "This is an undeniable fact: if all you're interested in is what your particular data tell you, frequentism's answer, the confidence interval (and the closely associated $p$-values) is useless.\n",
      "\n",
      "Unfortunately, most people using the confidence interval don't seem to realize this. They treat the confidence interval as if it's a (Bayesian) credible region, **but it demonstrably is not.** This oversight can be forgiven for the statistical layman, as even trained statisticians will often make the mistake of treating a confidence interval as if it is a credibile region. I think the reason this mistake is so common is that in many cases (as I showed above) the CI and the CR happen to coincide.  Frequentism gives the result you want in this case, **but only because it accidentally coincides with the Bayesian answer in simple situations.**\n",
      "\n",
      "Now, I am certainly not the first person to put things this way. The Physicist [Edwin Thompson Jaynes](http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes) was known as an ardent defender of Bayesianism; one of my main inspirations for this post is his 1976 paper, *Confidence Intervals vss Bayesian Intervals* ([pdf](http://bayes.wustl.edu/etj/articles/confidence.pdf)). More recently, statistician and blogger [W.M. Briggs](http://wmbriggs.com/) posted a diatribe on arXiv called [*It's Time To Stop Teaching Frequentism to Non-Statisticians](http://arxiv.org/abs/1201.2590) which brings up this same point.\n",
      "\n",
      "But in case this philosophical rant has not convinced you, I want to walk through a more complicated problem in which the difference between the two approaches becomes undisputable."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 2: Jaynes' Truncated Exponential"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def p(x, theta):\n",
      "    return (x > theta) * np.exp(theta - x)\n",
      "\n",
      "x = np.linspace(0, 20, 1000)\n",
      "plt.fill(x, p(x, 10), alpha=0.3)\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('p(x)');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAuBiIjuYCEQEREA4P8Bwa35RiSrD9QAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1065cea90>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From [Jaynes, 1976](https://bayes.wustl.edu/etj/articles/confidence.pdf) Imagine we have the following truncated exponential model:\n",
      "\n",
      "$$\n",
      "p(x~|~\\theta) = \\left\\{\n",
      "\\begin{array}{lll}\n",
      "\\exp(\\theta - x) &,& x > \\theta\\\\\n",
      "0                &,& x < \\theta\n",
      "\\end{array}\n",
      "\\right\\}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imagine now that we've observed some data, $D = \\{10, 11, 12\\}$, and we want to infer the value of $\\theta$ from this data. We'll explore four approaches to this below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Common Sense Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One general rule: before computing anything, you should think about the problem you're solving and guess what a reasonable solution might be. We'll start with that here.\n",
      "\n",
      "Thinking about the problem, the hard cutoff in the probability distribution leads to one simple observation:\n",
      "\n",
      "**$\\theta$ must be less than the smallest observed value**.\n",
      "\n",
      "This is immediately obvious on examination: the probability of seeing a value less than $\\theta$ is zero.  Thus, a model with $\\theta$ greater than any observed value is impossible, assuming our model is correct. Our fundamental assumption in both Bayesianism and frequentism is that the model is correct, so in this case, we can immediately write our common sense condition:\n",
      "\n",
      "$$\n",
      "\\theta < \\min(D)\n",
      "$$\n",
      "\n",
      "or, in the particular case of $D = \\{10, 11, 12\\}$,\n",
      "\n",
      "$$\n",
      "\\theta < 10\n",
      "$$\n",
      "\n",
      "With this in mind, let's go on to some quantitative approaches based on Frequentism and Bayesianism."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Frequentist approach #1: Sampling Distribution via the Normal Approximation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The population mean is given by\n",
      "\n",
      "$$\n",
      "\\begin{array}{ll}\n",
      "E(x) &= \\int_0^\\infty xp(x)dx\\\\\n",
      "     &= \\theta + 1\n",
      "     \\end{array}\n",
      "$$\n",
      "\n",
      "So, using the sample mean as the point estimate of $E(x)$, we have an unbiased estimator for $\\theta$ given by\n",
      "\n",
      "$$\n",
      "\\hat{\\theta} = \\frac{1}{N} \\sum_{i=1}^N (x_i + 1)\n",
      "$$\n",
      "\n",
      "For large $N$, by the central limit theorem, the sampling distribution of $\\hat{\\theta}$ will approach normal with variance $\\sigma^2 = 1 / N$, and we can write our 95% (i.e. 2$\\sigma$) confidence interval as\n",
      "\n",
      "$$\n",
      "CI_{\\rm large~N} = \\left(\\hat{\\theta} - 2 N^{-1/2},~\\hat{\\theta} + 2 N^{-1/2}\\right)\n",
      "$$\n",
      "\n",
      "Let's write a function which will compute this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import erfinv\n",
      "\n",
      "def approx_CI(D, sig=0.95):\n",
      "    \"\"\"Approximate truncated exponential confidence interval\"\"\"\n",
      "    # use erfinv to convert percentage to number of sigma\n",
      "    Nsigma = np.sqrt(2) * erfinv(sig)\n",
      "    D = np.asarray(D)\n",
      "    N = D.size\n",
      "    theta_hat = np.mean(D + 1)\n",
      "    return [theta_hat - Nsigma / np.sqrt(N),\n",
      "            theta_hat + Nsigma / np.sqrt(N)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 310
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = [10, 11, 12]\n",
      "print(\"theta_hat:       {0:.1f}\".format(np.mean(D) + 1))\n",
      "print(\"approximate CI: ({0:.1f}, {1:.1f})\".format(*approx_CI(D)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "theta_hat:       12.0\n",
        "approximate CI: (10.9, 13.1)\n"
       ]
      }
     ],
     "prompt_number": 311
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We immediately see an issue. By our simple common sense argument, we've determined that it is impossible for $\\theta$ to be greater than 10, yet the 95% confidence interval is entirely above this!  But let's not write-off frequentism here just yet: the above computation is based on a large-$N$ approximation, and we have a relatively paltry $N = 3$. Maybe this will be improved if we do the more computationally intensive exact approach. Let's try it:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Frequentist approach #2: Exact Sampling Distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For small $N$, the normal approximation will not apply, and we must instead compute the confidence integral from the sampling distribution, which is the distribution of the sum of $N$ variables each distributed according to $p(\\theta)$, above. The sampling distribution is the convolution of the input distributions, so we can exploit the [convolution theorem](http://en.wikipedia.org/wiki/Convolution_theorem) and use the method of [characteristic functions](http://en.wikipedia.org/wiki/Characteristic_function_(probability_theory) to find the following sampling distribution for the sum of $N$ variables distributed according to the $p(x~|~\\theta)$ above:\n",
      "\n",
      "$$\n",
      "f(\\theta~|~D) \\propto\n",
      "\\left\\{\n",
      "\\begin{array}{lll}\n",
      "z^{N - 1}\\exp(-z) &,& z > 0\\\\\n",
      "0 &,& z < 0\n",
      "\\end{array}\n",
      "\\right\\}\n",
      ";~ z = N(\\hat{\\theta} + 1 - \\theta)\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To compute the 95% confidence interval, we can start by computing the cumulative distribution: we integrate $f(\\theta~|~D)$ from $0$ to $\\theta$. We can make use of the expression for the [incomplete gamma function](http://en.wikipedia.org/wiki/Incomplete_gamma_function):\n",
      "\n",
      "$$\n",
      "\\Gamma(a, x) = \\int_x^\\infty t^{a - 1}e^{-t} dt\n",
      "$$\n",
      "\n",
      "which looks strikingly similar to our $f(\\theta)$.\n",
      "\n",
      "Using this to perform the integral, we find that the cumulative distribution is given by\n",
      "\n",
      "$$\n",
      "F(\\theta~|~D) = \\frac{1}{\\Gamma(N)}\\left[ \\Gamma\\left(N, \\max[0, N(\\hat{\\theta} + 1 - \\theta)]\\right) - \\Gamma\\left(N,~N(\\hat{\\theta} + 1)\\right)\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are an infinite number of possible 95% confidence intervals given by the solutions to the following equation:\n",
      "\n",
      "$$\n",
      "F(\\theta_2~|~D) - F(\\theta_1~|~D) = 0.95\n",
      "$$\n",
      "\n",
      "What we want is the shortest of these intervals. We'll add the constraint that the probability density is equal at either side of the interval:\n",
      "\n",
      "$$\n",
      "f(\\theta_2~|~D) = f(\\theta_1~|~D)\n",
      "$$\n",
      "\n",
      "As a side note, Jaynes claims that this ensures the shortest possible interval, but I'm not sure how to prove that.\n",
      "Solving this system of two nonlinear equations gives us the confidence interval. Let's compute this numerically:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import gammaincc\n",
      "from scipy import optimize\n",
      "\n",
      "\n",
      "def exact_CI(D, frac=0.95):\n",
      "    \"\"\"Exact truncated exponential confidence interval\"\"\"\n",
      "    D = np.asarray(D)\n",
      "    N = D.size\n",
      "    theta_hat = np.mean(D) + 1\n",
      "\n",
      "    def f(theta, D):\n",
      "        z = theta_hat + 1 - theta\n",
      "        return (z > 0) * z ** (N - 1) * np.exp(-N * z)\n",
      "\n",
      "    def F(theta, D):\n",
      "        theta_hat = np.mean(D + 1)\n",
      "        return gammaincc(N, np.maximum(0, N * (theta_hat + 1 - theta))) - gammaincc(N, N * (theta_hat + 1))\n",
      "    \n",
      "    def eqns(CI, D):\n",
      "        \"\"\"Equations which should be equal to zero\"\"\"\n",
      "        theta1, theta2 = CI\n",
      "        return (F(theta2, D) - F(theta1, D) - frac,\n",
      "                f(theta2, D) - f(theta1, D))\n",
      "    \n",
      "    guess = approx_CI(D, 0.68) # use 1-sigma interval as a guess\n",
      "    result = optimize.root(eqns, guess, args=(D,))\n",
      "    if not result.success:\n",
      "        print \"result did not converge!\"\n",
      "    return result.x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 312
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a sanity check, let's make sure that the exact and approximate confidence intervals match for a large number of points:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(42)\n",
      "Dlarge = 10 + np.random.random(100)\n",
      "print \"approx: ({0:.3f}, {1:.3f})\".format(*approx_CI(Dlarge))\n",
      "print \"exact: ({0:.3f}, {1:.3f})\".format(*exact_CI(Dlarge))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "approx: (11.274, 11.666)\n",
        "exact: (11.272, 11.663)\n"
       ]
      }
     ],
     "prompt_number": 313
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected, the approximate solution is very close to the exact solution for large $N$, which gives us confidence that we're computing the right thing.\n",
      "\n",
      "Let's return to our 3-point dataset and see the results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"theta_hat:       {0:.1f}\".format(np.mean(D) + 1))\n",
      "print(\"approximate CI: ({0:.1f}, {1:.1f})\".format(*approx_CI(D)))\n",
      "print(\"exact CI:       ({0:.1f}, {1:.1f})\".format(*exact_CI(D)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "theta_hat:       12.0\n",
        "approximate CI: (10.9, 13.1)\n",
        "exact CI:       (10.9, 12.9)\n"
       ]
      }
     ],
     "prompt_number": 314
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The exact confidence interval is slightly different than the approximate one, but still reflects the same problem: **we know that $\\theta$ can't be greater than 10, yet the 95% confidence interval is entirely in this forbidden region**!\n",
      "\n",
      "Let's see if Bayes can do better."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4. Bayesian Credibility Interval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the Bayesian solution, we start by writing Bayes' rule:\n",
      "\n",
      "$$\n",
      "p(\\theta~|~D) = \\frac{p(D~|~\\theta)p(\\theta)}{P(D)}\n",
      "$$\n",
      "\n",
      "Using a constant prior $p(\\theta)$, and with the likelihood\n",
      "\n",
      "$$\n",
      "p(D~|~\\theta) = \\prod_{i=1}^N p(x~|~\\theta)\n",
      "$$\n",
      "\n",
      "we find\n",
      "\n",
      "$$\n",
      "p(\\theta~|~D) = \\left\\{\n",
      "\\begin{array}{lll}\n",
      "N\\exp\\left[N(\\theta - \\min(D))\\right] &,& \\theta < \\min(D)\\\\\n",
      "0                &,& \\theta > \\min(D)\n",
      "\\end{array}\n",
      "\\right\\}\n",
      "$$\n",
      "\n",
      "where $\\min(D)$ is the smallest value in the data $D$, which enters because of the truncation of $p(x~|~\\theta)$.\n",
      " Because $p(\\theta~|~D)$ increases exponentially up to the cutoff, the shortest 95% credibility interval $(\\theta_1, \\theta_2)$ will be given by\n",
      "\n",
      "$$\n",
      "\\theta_2 = \\min(D)\n",
      "$$\n",
      "\n",
      "and $\\theta_1$ given by the solution to the equation\n",
      "\n",
      "$$\n",
      "\\int_{\\theta_1}^{\\theta_2} N\\exp[N(\\theta - \\theta_2)]d\\theta = f\n",
      "$$\n",
      "\n",
      "this can be solved by evaluating the integral, which gives\n",
      "\n",
      "$$\n",
      "\\theta_1 = \\theta_2 + \\frac{\\log(1 - f)}{N}\n",
      "$$\n",
      "\n",
      "let's write a function which computes this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bayes_CR(D, frac=0.95):\n",
      "    \"\"\"Bayesian Credibility Region\"\"\"\n",
      "    D = np.asarray(D)\n",
      "    N = float(D.size)\n",
      "    theta2 = D.min()\n",
      "    theta1 = theta2 + np.log(1. - frac) / N\n",
      "    return theta1, theta2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 315
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have this Bayesian method, we can compare the results of the four methods:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"common sense:         theta < {0:.1f}\".format(np.min(D)))\n",
      "print(\"frequentism (approx): 95% CI = ({0:.1f}, {1:.1f})\".format(*approx_CI(D)))\n",
      "print(\"frequentism (exact):  95% CI = ({0:.1f}, {1:.1f})\".format(*exact_CI(D)))\n",
      "print(\"Bayesian:             95% CR = ({0:.1f}, {1:.1f})\".format(*bayes_CR(D)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "common sense:         theta < 10.0\n",
        "frequentism (approx): 95% CI = (10.9, 13.1)\n",
        "frequentism (exact):  95% CI = (10.9, 12.9)\n",
        "Bayesian:             95% CR = (9.0, 10.0)\n"
       ]
      }
     ],
     "prompt_number": 316
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So what do we find is that the Bayesian result agrees with our common sense, while the frequentist approach does not. But don't be fooled: **the frequentist result is, in fact, correct!** The problem is this:\n",
      "\n",
      "**The frequentist credibility interval is not answering the question that we think we're asking.**\n",
      "\n",
      "This, in my mind, is the fundamental reason that frequentism should be avoided in scientific research. Let's discuss why."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Rub: Confidence vs. Credibility"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "blah"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}