{
 "metadata": {
  "name": "",
  "signature": "sha256:c1d189b821aa456a1dbb7e6f744bed29e351440ed3437ccf3a6354b2555b3c57"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequentism vs Bayesianism II: When Results Differ"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a [recent post](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) I gave a brief practical introduction to frequentism and Bayesianism as they relate to the analysis of scientific data. One aspect of that introduction was that I showed how the frequentist and Bayesian approaches to a couple simple problems basically gave the same results.\n",
      "\n",
      "Here I want to highlight some simple situations where the results differ."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Bayesian Billiard Game"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'll start with an example that, in one form or another, dates all the way back to the [1763 paper](http://www.stat.ucla.edu/history/essay.pdf) written by Thomas Bayes himself. The particular version of this problem I use here is borrowed from [Eddy 2004](ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdf).\n",
      "\n",
      "As usual, it starts with Alice and Bob. And today they're playing a game, which goes like this:\n",
      "\n",
      "Behind a curtain there is a billiard table. An assistant rolls a first ball down the table, and marks where it lands. Once this mark is in place, the assistant randomly rolls new balls down the table; if the ball lands to the left of the mark, Alice gets a point. If the ball lands to the right of the mark, Bob gets a point.  We can assume for the sake of example that the person rolling the balls is unbiased: that is, the balls have an equal chance of ending up anywhere on the table.  The first person to reach **six points** wins the game.\n",
      "\n",
      "The important thing here is that to Alice and Bob, the table and the rolls are basically a black box: they know nothing about the rolls themselves, only the result after each roll. So if the first ball (which generates the mark) settles far to the left, then Alice will have only a small chance of getting a point. If the first ball settles far to the left, then Bob will only have a small chance of getting a point, and they won't know this until the results start coming in.\n",
      "\n",
      "Given this setup, here is the question to solve:\n",
      "\n",
      "> In a particular game, after eight rolls, Alice has five points and Bob has three points. What is the probability that Bob will go on to win the game?\n",
      "\n",
      "Setting back and reasoning about this, you might think that because Alice received five of the eight points, the marker ball must favor her. And given this, it's more likely that the next roll will go her way as well. And she has three opportunities to get a favorable roll before Bob can win; she seems to have clinched it.  But, **quantitatively**, what is the probability that Bob will prevail?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A Naive Frequentist Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Someone following a frequentist approach might reason as follows:\n",
      "\n",
      "To determine the result, we need to know the probability $p$ that a roll lands in Alice's favor.  We can estimate this probability using maximum likelihood, which results in\n",
      "$$\n",
      "\\hat{p} = 5/8\n",
      "$$\n",
      "Assuming this maximum likelihood probability, we can compute the probability that Bob will win, which is given by:\n",
      "$$\n",
      "P(B) = (1 - \\hat{p})^3\n",
      "$$\n",
      "Thus, we find that the following estimate of the probability:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p_hat = 5. / 8.\n",
      "freq_prob = (1 - p_hat) ** 3\n",
      "print(\"Naive Frequentist Probability of Bob Winning: {0:.2f}\".format(freq_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Naive Frequentist Probability of Bob Winning: 0.05\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In terms of gambling odds like those used at horse races, we'd give Bob the following odds of winning:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Odds on Bob winning: {0:.0f} to 1\".format(1 / freq_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Odds on Bob winning: 19 to 1\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we've estimated that Alice will win about 19 times for each time Bob wins.\n",
      "\n",
      "But is this Maximum Likelihood/hybrid approach correct? Let's explore some other approaches to figure this out."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Monte Carlo Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another way we can answer this problem is to brute-force it. We can use a *Monte Carlo* simulation in which we run simulations of these games many times and ask how often Bob wins in this particular situation.  This problem is especially simple because so many of the distributions are uniform.  We can use the ``numpy`` package to do this as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.random.seed(0)\n",
      "\n",
      "# play 100000 games with randomly-drawn p, between 0 and 1\n",
      "p = np.random.random(100000)\n",
      "\n",
      "# each game needs at most 11 rolls for one player to reach 6 wins\n",
      "rolls = np.random.random((11, len(p)))\n",
      "\n",
      "# count the cumulative wins for Alice and Bob at each roll\n",
      "Alice_count = np.cumsum(rolls < p, 0)\n",
      "Bob_count = np.cumsum(rolls >= p, 0)\n",
      "\n",
      "# sanity check: total number of wins should equal number of rolls\n",
      "total_wins = Alice_count + Bob_count\n",
      "assert np.all(total_wins.T == np.arange(1, 12))\n",
      "print(\"(Sanity check passed)\")\n",
      "\n",
      "# determine number of games which meet our criterion of (A wins, B wins)=(5, 3)\n",
      "# this means Bob's count at eight rolls must equal 3\n",
      "good_games = Bob_count[7] == 3\n",
      "print(\"Number of suitable games: {0}\".format(good_games.sum()))\n",
      "\n",
      "# truncate our results to consider only these games\n",
      "Alice_count = Alice_count[:, good_games]\n",
      "Bob_count = Bob_count[:, good_games]\n",
      "\n",
      "# determine which of these games Bob won\n",
      "# to win, he must reach six wins after 11 rolls.\n",
      "bob_won = np.sum(Bob_count[10] == 6)\n",
      "print(\"Number of these games Bob won: {0}\".format(bob_won.sum()))\n",
      "\n",
      "# compute probability\n",
      "mc_prob = bob_won.sum() * 1. / good_games.sum()\n",
      "print(\"Monte Carlo Probability of Bob winning: {0:.2f}\".format(mc_prob))\n",
      "print(\"MC Odds on Bob winning: {0:.0f} to 1\".format(1 / mc_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sanity check passed)\n",
        "Number of suitable games: 11068\n",
        "Number of these games Bob won: 979\n",
        "Monte Carlo Probability of Bob winning: 0.09\n",
        "MC Odds on Bob winning: 11 to 1\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Monte Carlo approach gives 11-to-1 odds on Bob, rather than the 19-to-1 odds of the naive frequentist approach. Apparently our naive approach was a bit too naive.\n",
      "\n",
      "Could this difference be due to the variance of the monte carlo approach?  Well, as an incredibly rough approximation, we can use a Poisson root-N estimate the confidence level for this estimated probability: $\\epsilon\\sim\\sqrt{1000} / 10000 \\sim 0.003$, meaning that the Naive frequentist result of 0.05 is easily ruled out."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bayesian Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also approach this problem from a Bayesian standpoint. This is a bit more involved, and requires us to first define some notation.\n",
      "\n",
      "We'll consider the following random variables:\n",
      "\n",
      "- $B$ = Bob Wins\n",
      "- $D$ = observed data, i.e. $D = (n_A, n_B) = (5, 3)$\n",
      "- $p$ = unknown probability that a ball lands on Alice's side during the current game\n",
      "\n",
      "We want to compute $P(B~|~D)$; that is, the probability that Bob wins given our observation that she currently has five points to Bob's three."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Building our Bayesian Expression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To compute this result, we will manipulate our desired quantity $P(B~|~D)$ until we can express it in terms of other quantities that we can compute."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll start by applying the following identity of conditional probability:\n",
      "\n",
      "$$\n",
      "P(X) \\equiv \\int P(X~|~Y)P(Y)~dY\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Expanding $P(B~|~D)$ in terms of the unknown quantity $p$, we have\n",
      "\n",
      "$$\n",
      "P(B~|~D) = \\int P(B~|~p, D) P(p~|~D) dp\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we use Bayes' rule to rewrite $P(p~|~D)$:\n",
      "\n",
      "$$\n",
      "P(B~|~D) = \\int P(B~|~p, D) \\frac{P(D~|~p)P(p)}{P(D)} dp\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, using the same conditional probability identity we used above, we can expand $P(D)$ to find:\n",
      "\n",
      "$$\n",
      "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the desired probability is expressed in terms of three quantities that we can compute. Let's look at each of these in turn:\n",
      "\n",
      "- $P(B~|~p,D)$: This term is exactly the frequentist result we derived above. In words: given our probability $p$ and the fact that Alice has won 5 times and Bob 3 times, what is the probability that Bob will go on to six wins?  Bob needs three wins in a row, or $P(A~|~p,D) = (1 - p) ^ 3$.\n",
      "- $P(D~|~p)$: this is another easy-to-compute term. In words: given a probability $p$, what is the likelihood of exactly 5 positive outcomes out of eight trials? The answer comes from the well-known [Binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution): in this case $P(D~|~p) \\propto p^5 (1-p)^3$\n",
      "- $P(p)$: this is our prior on the probability $p$. By construction, we assume that $p$ is evenly drawn between 0 and 1.  That is, $P(p) \\propto 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Putting this all together, canceling some terms, and simplifying a bit, we find\n",
      "$$\n",
      "P(B~|~D) = \\frac{\\int (1 - p)^6 p^5 dp}{\\int (1 - p)^3 p^5 dp}\n",
      "$$\n",
      "where both integrals are evaluated from 0 to 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These integrals might look a bit difficult, but they are examples of the well-known [Beta Function](http://en.wikipedia.org/wiki/Beta_function):\n",
      "$$\n",
      "\\beta(n, m) = \\int_0^1 (1 - p)^{n - 1} p^{m - 1}\n",
      "$$\n",
      "The Beta function can be expressed in terms of gamma functions (i.e. factorials), but for simplicity we'll compute them directly using Scipy's beta implementation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import beta\n",
      "bayes_prob = beta(7, 6) / beta(4, 6)\n",
      "\n",
      "print(\"P(B|D) = {0:.2f}\".format(bayes_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(B|D) = 0.09\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we see that the Bayesian result gives us 0.09, similar to what we saw in the Monte Carlo simulation, and different than the \"Naive\" frequentist approach.  That is, the Bayesian approach gives us the correct answer as confirmed by the Monte Carlo simulations!\n",
      "\n",
      "Just to put the point home, let's look at the odds of Alice winning as computed by the Frequentist and Bayesian approaches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Frequentist odds on Bob winning: {0:.0f} to 1\".format(1. / freq_prob))\n",
      "print(\"Bayesian odds on Bob winning: {0:.0f} to 1\".format(1. / bayes_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Frequentist odds on Bob winning: 19 to 1\n",
        "Bayesian odds on Bob winning: 11 to 1\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If Alice and Bob are using these computations to inform their bets on the results of the game, the different approaches will lead to wildly different betting strategies!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Is Frequentism Wrong?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before my comment thread explodes with responses from irritated statisticians, I should make it clear that I've been a bit unfair here. The naive frequentist approach used above does give the wrong answer, but that's more a factor of it being **naive** than it being **frequentist**. I'm certain that there exists a frequentist approach to this problem which yields the correct answer, but the fact is *I don't know what it is* (and if anyone cares to enlighten me on this, I'd greatly appreciate it!)\n",
      "\n",
      "The take-home here is not that frequentism is incorrect, but that using frequentism **it's much easier to arrive at a wrong result using frequentist methods than using Bayesian methods**.  Frequentism often hides its assumptions behind rules and approximations. Certainly, if you have a PhD in statistics, you should know enough to recognize when those assumptions do and do not apply. But for those of us without that expertise, Bayesianism is much more forgiving: Bayesianism has the virtue of putting all of its assumptions in the open, and (in general) as long as your algebra is correct, those assumptions won't hide themselves. The naive frequentist approach I used above was, at first blush, as convincing as it was incorrect, and that makes it problematic for use in a field where researchers (and their peer reviewers) are not necessarily experts in statistics."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Frequentism is Hard"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I think one of the biggest misconceptions in the frequentism/Bayesianism debat is that frequentist computations are easy, while Bayesian computations are hard. This may well be true in terms of computational complexity. In terms of expertise required to arrive at correct results, however, I find the opposite to be true: correctly applying Bayesianism in real life requires much less special expertise than correctly applying frequentism.\n",
      "\n",
      "So you might say that *I'm a Bayesian because I don't know enough about statistics to be a good frequentist!*\n",
      "\n",
      "Not that Bayesiansim doesn't have its weaknesses: as I've pointed out, it often requires its own subtlety and expertise: as discussed in the previous post, choosing a suitable prior can prove to be difficult. Also, real-life Bayesian results tend to require high-dimensional integrations over complicated probability distributions, and the MCMC approaches to these high-dimensional calculations often require an expertise of their own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "In Summary: The Moral of the Story"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Synthesizing the last two posts, here's the distillation of what I think you should know about frequentism and Bayesiansim:\n",
      "\n",
      "1. The difference between frequentism and Bayesianism is a difference of philosophy, in particular about the fundamental definition of probability.\n",
      "2. Frequentist approaches tend to be derived from well-justified assumptions about the distribution of data which allow analytic computation of results; unless you understand these assumptions, frequentist techniques can be easy to misuse.\n",
      "3. Bayesian approaches tend to give up these implicit assumptions and is in this sense more \"rigorous\", but this rigor often requires explicit assumptions about the prior, as well as increased computational cost."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}