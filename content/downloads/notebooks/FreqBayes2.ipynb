{
 "metadata": {
  "name": "",
  "signature": "sha256:3b29d1ce5955cb24553e874ddf207d141bfd52008c5ffaa076e64ba7a3e1dbbc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Frequentism vs Bayesianism II: When Results Differ"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a [previous post](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) I gave a brief practical introduction to frequentism and Bayesianism as they relate to the analysis of scientific data. One aspect of that introduction was that I showed how the frequentist and Bayesian approaches to a couple simple problems basically gave the same results.\n",
      "\n",
      "While it is easy to show that the two approaches are often equivalent for simple problems, they can diverge greatly for more complicated problems. I've found that in practice, this divergence makes itself most clear in two different situations:\n",
      "\n",
      "1. The handling of nuisance parameters\n",
      "2. The subtle (and often overlooked) difference between frequentist confidence intervals and Bayesian credible intervals\n",
      "\n",
      "The second point is a bit more philosophical and in-depth; I'm going to save it for a later post and focus here on the first point: the difference between frequentist and Bayesian treatment of nuisance parameters. Though I tried my best to stay impartial in the previous post, here you'll start to see my leanings toward the Bayesian approach. Consider this a warmup for when I get around to addressing point number 2: that will likely get downright polemical."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Nuisance Parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A nuisance parameter is any quantity whose value is not relevant to the goal of an analysis, but is nevertheless required to determine some quantity of interest. For example, in the [second application](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/#Varying-Photon-Counts:-The-Bayesian-Approach) worked-through in the previous post, we estimated both the mean $\\mu$ and intrinsic scatter $\\sigma$ for the observed photons. In reality, we may only be interested in $\\mu$, the mean of the distribution. In this case $\\sigma$ is a nuisance parameter: it is not of immediate interest, but is nevertheless an essential part of determining the final estimate of $\\mu$, the parameter we care about."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "An Example: The Bayesian Billiard Game"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'll start with an example of nuisance parameters that, in one form or another, dates all the way back to the posthumous [1763 paper](http://www.stat.ucla.edu/history/essay.pdf) written by Thomas Bayes himself. The particular version of this problem I use here is borrowed from [Eddy 2004](ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdf).\n",
      "\n",
      "The setting is a rather contrived game in which Alice and Bob bet on the outcome of a process they can't directly observe:\n",
      "\n",
      "Alice and Bob enter a room. Behind a curtain there is a billiard table, which they cannot see, but their friend Carol can. Carol rolls a ball down the table, and marks where it lands. Once this mark is in place, Carol randomly rolls new balls down the table. If the ball lands to the left of the mark, Alice gets a point; if it lands to the right of the mark, Bob gets a point.  We can assume for the sake of example that Carol's rolls are unbiased: that is, the balls have an equal chance of ending up anywhere on the table.  The first person to reach **six points** wins the game.\n",
      "\n",
      "Here the location of the mark (determined by the first roll) is a nuisance parameter. It is unknown, and not of immediate interest, but it clearly must be accounted for when predicting the outcome of subsequent rolls. If the first roll settles far to the right, then all subsequent rolls will favor Alice. If it settles far to the right, Bob will be favored instead.\n",
      "\n",
      "Given this setup, here is the question we ask of ourselves:\n",
      "\n",
      "> In a particular game, after eight rolls, Alice has five points and Bob has three points. What is the probability that Bob will go on to win the game?\n",
      "\n",
      "Intuitively, you probably realize that because Alice received five of the eight points, the marker ball likely favors her. And given this, it's more likely that the next roll will go her way as well. And she has three opportunities to get a favorable roll before Bob can win; she seems to have clinched it.  But, **quantitatively**, what is the probability that Bob will squeak-out a win?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A Naive Frequentist Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Someone following a frequentist approach might reason as follows:\n",
      "\n",
      "To determine the result, we need an intermediate estimate of where the marker sits: we can quantify this as a probability $p$ that any given roll lands in Alice's favor.  Because five balls out of eight fell in Alice's favor, we can quickly show that the maximum likelihood estimate of $p$ is given by:\n",
      "\n",
      "$$\n",
      "\\hat{p} = 5/8\n",
      "$$\n",
      "\n",
      "(This result follows in a straightforward manner from the [binomial likelihood](http://en.wikipedia.org/wiki/Binomial_distribution)) Assuming this maximum likelihood probability, we can compute the probability that Bob will win, which is given by:\n",
      "\n",
      "$$\n",
      "P(B) = (1 - \\hat{p})^3\n",
      "$$\n",
      "\n",
      "Thus, we find that the following estimate of the probability:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p_hat = 5. / 8.\n",
      "freq_prob = (1 - p_hat) ** 3\n",
      "print(\"Naive Frequentist Probability of Bob Winning: {0:.2f}\".format(freq_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Naive Frequentist Probability of Bob Winning: 0.05\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words, we'd give Bob the following odds of winning:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Odds on Bob winning: {0:.0f} to 1\".format((1. - freq_prob) / freq_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Odds on Bob winning: 18 to 1\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we've estimated that Alice will win about 18 times for each time Bob wins using frequentist ideas. Let's try a Bayesian approach next."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bayesian Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also approach this problem from a Bayesian standpoint. This is slightly more involved, and requires us to first define some notation.\n",
      "\n",
      "We'll consider the following random variables:\n",
      "\n",
      "- $B$ = Bob Wins\n",
      "- $D$ = observed data, i.e. $D = (n_A, n_B) = (5, 3)$\n",
      "- $p$ = unknown probability that a ball lands on Alice's side during the current game\n",
      "\n",
      "We want to compute $P(B~|~D)$; that is, the probability that Bob wins given our observation that she currently has five points to Bob's three.\n",
      "\n",
      "The general Bayesian method of treating nuisance parameters is *marginalization*, or integrating the joint probability over the entire range of the nuisance parameter. In this case, that means that we will first calculate the joint distribution\n",
      "$$\n",
      "P(B,p~|~D)\n",
      "$$\n",
      "and then marginalize over $p$ using the following identity:\n",
      "$$\n",
      "P(B~|~D) \\equiv \\int_{-\\infty}^\\infty P(B,p~|~D) {\\mathrm d}p\n",
      "$$\n",
      "This identity follows from the definition of conditional probability, and the law of total probability: that is, it is a fundamental consequence of probability axioms which is always true."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Building our Bayesian Expression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To compute this result, we will manipulate the above expression for $P(B~|~D)$ until we can express it in terms of other quantities that we can compute."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll start by applying the following definition of [conditional probability](http://en.wikipedia.org/wiki/Conditional_probability#Definition) to expand the term $P(B,p~|~D)$:\n",
      "\n",
      "$$\n",
      "P(B~|~D) = \\int P(B~|~p, D) P(p~|~D) dp\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we use [Bayes' rule](http://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite $P(p~|~D)$:\n",
      "\n",
      "$$\n",
      "P(B~|~D) = \\int P(B~|~p, D) \\frac{P(D~|~p)P(p)}{P(D)} dp\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, using the same probability identity we started with, we can expand $P(D)$ in the denominator to find:\n",
      "\n",
      "$$\n",
      "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the desired probability is expressed in terms of three quantities that we can compute. Let's look at each of these in turn:\n",
      "\n",
      "- $P(B~|~p,D)$: This term is exactly the frequentist likelihood we used above. In words: given a probability $p$ and the fact that Alice has won 5 times and Bob 3 times, what is the probability that Bob will go on to six wins?  Bob needs three wins in a row, i.e. $P(B~|~p,D) = (1 - p) ^ 3$.\n",
      "- $P(D~|~p)$: this is another easy-to-compute term. In words: given a probability $p$, what is the likelihood of exactly 5 positive outcomes out of eight trials? The answer comes from the well-known [Binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution): in this case $P(D~|~p) \\propto p^5 (1-p)^3$\n",
      "- $P(p)$: this is our prior on the probability $p$. By the problem definition, we can assume that $p$ is evenly drawn between 0 and 1.  That is, $P(p) \\propto 1$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Putting this all together, canceling some terms, and simplifying a bit, we find\n",
      "$$\n",
      "P(B~|~D) = \\frac{\\int (1 - p)^6 p^5 dp}{\\int (1 - p)^3 p^5 dp}\n",
      "$$\n",
      "where both integrals are evaluated from 0 to 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These integrals might look a bit difficult, but they are examples of the well-known [Beta Function](http://en.wikipedia.org/wiki/Beta_function):\n",
      "$$\n",
      "\\beta(n, m) = \\int_0^1 (1 - p)^{n - 1} p^{m - 1}\n",
      "$$\n",
      "The Beta function can be further expressed in terms of gamma functions (i.e. factorials), but for simplicity we'll compute them directly using Scipy's implementation of the beta function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import beta\n",
      "bayes_prob = beta(6 + 1, 5 + 1) / beta(3 + 1, 5 + 1)\n",
      "\n",
      "print(\"P(B|D) = {0:.2f}\".format(bayes_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(B|D) = 0.09\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The associated odds are the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Bayesian odds on Bob winning: {0:.0f} to 1\".format((1. - bayes_prob) / bayes_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Bayesian odds on Bob winning: 10 to 1\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we see that the Bayesian result gives us 10 to 1 odds, which is quite different than the 18 to 1 odds found using the frequentist approach. So which one is correct?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A Brute Force/Monte Carlo Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this type of well-defined situation, it is actually relatively easy to use a monte carlo simulation to determine the correct answer. This is essentially a brute-force tabulation of possible outcomes: we generate a large number of random games, and simply count the fraction of suitable games that Bob goes on to win. The current problem is especially simple because so many of the distributions are uniform.  We can use the ``numpy`` package to do this as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.random.seed(0)\n",
      "\n",
      "# play 100000 games with randomly-drawn p, between 0 and 1\n",
      "p = np.random.random(100000)\n",
      "\n",
      "# each game needs at most 11 rolls for one player to reach 6 wins\n",
      "rolls = np.random.random((11, len(p)))\n",
      "\n",
      "# count the cumulative wins for Alice and Bob at each roll\n",
      "Alice_count = np.cumsum(rolls < p, 0)\n",
      "Bob_count = np.cumsum(rolls >= p, 0)\n",
      "\n",
      "# sanity check: total number of wins should equal number of rolls\n",
      "total_wins = Alice_count + Bob_count\n",
      "assert np.all(total_wins.T == np.arange(1, 12))\n",
      "print(\"(Sanity check passed)\")\n",
      "\n",
      "# determine number of games which meet our criterion of (A wins, B wins)=(5, 3)\n",
      "# this means Bob's win count at eight rolls must equal 3\n",
      "good_games = Bob_count[7] == 3\n",
      "print(\"Number of suitable games: {0}\".format(good_games.sum()))\n",
      "\n",
      "# truncate our results to consider only these games\n",
      "Alice_count = Alice_count[:, good_games]\n",
      "Bob_count = Bob_count[:, good_games]\n",
      "\n",
      "# determine which of these games Bob won\n",
      "# to win, he must reach six wins after 11 rolls.\n",
      "bob_won = np.sum(Bob_count[10] == 6)\n",
      "print(\"Number of these games Bob won: {0}\".format(bob_won.sum()))\n",
      "\n",
      "# compute probability\n",
      "mc_prob = bob_won.sum() * 1. / good_games.sum()\n",
      "print(\"Monte Carlo Probability of Bob winning: {0:.2f}\".format(mc_prob))\n",
      "print(\"MC Odds on Bob winning: {0:.0f} to 1\".format((1. - mc_prob) / mc_prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(Sanity check passed)\n",
        "Number of suitable games: 11068\n",
        "Number of these games Bob won: 979\n",
        "Monte Carlo Probability of Bob winning: 0.09\n",
        "MC Odds on Bob winning: 10 to 1\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Monte Carlo approach gives 10-to-1 odds on Bob, which agrees with the Bayesian approach. Apparently, our naive frequentist approach above was flawed."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Discussion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example shows several different approaches to dealing with the presence of a nuisance parameter *p*. The Monte Carlo simulation gives us a close brute-force estimate of the true probability (assuming the validity of our assumptions), which the Bayesian approach matches. The naive frequentist approach, by utilizing a single maximum likelihood estimate of the nuisance parameter $p$, arrives at the wrong result.\n",
      "\n",
      "Before my comment thread explodes and I get ripped to shreds \u2013 again \u2013 on Reddit and Hacker News, I should emphasize that **this does not imply frequentism itself is incorrect**. The incorrect result above is more a matter of the approach being \"naive\" than it being \"frequentist\". There certainly exist frequentist methods for handling this sort of nuisance parameter \u2013 for example, it is theoretically possible to apply a transformation and conditioning of the data to isolate the dependence on $p$ \u2013 but I've not been able to find any approach to this particular problem that does not somehow take advantage of Bayesian-like marginalization.\n",
      "\n",
      "Another potential point of contention is that the question itself is posed in a way that is perhaps unfair to the classical, frequentist approach. A frequentist might instead hope to give the answer in terms of null tests or confidence intervals: that is, they might devise a procedure to construct limits which would provably bound the correct answer in $100\\times(1 - p)$ percent of similar trials, for some value of $p$ (say, 0.05). This might be classically correct, but it doesn't quite answer the question at hand. I'll leave discussion of the meaning of such confidence intervals for my follow-up post on the subject.\n",
      "\n",
      "There is one clear common point of these two potential frequentist responses: both require some degree of effort and/or special expertise; perhaps a suitable frequentist approach would be immediately obvious to someone with a PhD in statistics, but is most definitely *not* obvious to a statistical lay-person simply trying to answer the question at hand. In this sense, I think Bayesianism provides a better approach for this sort of problem: by simple algebraic manipulation of a few well-known axioms of probability within a Bayesian framework, we can straightforwardly arrive at the correct answer without need for other special expertise.\n",
      "\n",
      "We'll explore a more data-oriented example of dealing with nuisance parameters next."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}