{
 "metadata": {
  "name": "",
  "signature": "sha256:2af15b54d32578442a045c34f97f2a049fa99cff5f9de6653faf452d501d0646"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Frequentism and Bayesianism V: Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*This notebook originally appeared as a [post](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/) on the blog [Pythonic Perambulations](http://jakevdp.github.io). The content is BSD licensed.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*This post is part of a 5-part series: [Part I](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)  [Part II](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/)  [Part III](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/)  [Part IV](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/) [Part V -- TODO]()*\n",
      "\n",
      "*See also [Frequentism and Bayesianism: A Python-driven Primer](http://arxiv.org/abs/1411.5018), a peer-reviewed article partially based on this content.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Last year I wrote a series of posts comparing frequentist and Bayesian approaches to various problems:\n",
      "\n",
      "- In [Frequentism and Bayesianism I: a Practical Introduction](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) I gave an introduction to the main philosophical differences between frequentism and Bayesianism, and showed that for many common problems the two methods give basically the same point estimates.\n",
      "- In [Frequentism and Bayesianism II: When Results Differ](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/) I went into a bit more depth on when frequentism and Bayesianism start to diverge, particularly when it comes to the handling of nuisance parameters.\n",
      "- In [Frequentism and Bayesianism III: Confidence, Credibility, and why Frequentism and Science Don't Mix](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/) I talked about the subtle difference between frequentist confidence intervals and Bayesian credible intervals, and argued that in most scientific settings frequentism answers the wrong question.\n",
      "- In [Frequentism and Bayesianism IV: How to be a Bayesian in Python](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/) I compared three Python packages for doing Bayesian analysis via MCMC: [emcee](http://dan.iel.fm/emcee), [pymc](http://pymc-devs.github.io/pymc/), and [pystan](https://pystan.readthedocs.org/en/latest/).\n",
      "\n",
      "One important piece that I've not yet covered is the notion of *model selection*.\n",
      "That is the subject of this post."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model Fitting vs Model Selection\n",
      "\n",
      "The difference between model fitting and model selection sometimes causes confusion.\n",
      "By model, here, I mean a formula, usually with tunable parameters, which quantifies the likelihood of observing your data.\n",
      "\n",
      "For example, your model might consist of the statement, \"the $(x, y)$ observations come from a straight line, with known normal measurement errors $\\sigma_y$\".\n",
      "Labeling this model $M_1$, we can write:\n",
      "\n",
      "$$\n",
      "y_{M_1}(x;\\theta) = \\theta_0 + \\theta_1 x\\\\\n",
      "y \\sim \\mathcal{N}(y_{M_1}, \\sigma_y^2)\n",
      "$$\n",
      "\n",
      "where the second line indicates that the observed $y$ is normally distributed about the model value, with variance $\\sigma_y^2$.\n",
      "There are two tunable parameters to this model, represented by the vector $\\theta = [\\theta_0, \\theta_1]$ (i.e. the slope and intercept).\n",
      "\n",
      "Another model might consist of the statement \"the observations $(x, y)$ come from a quadratic curve, with known normal measurement errors $\\sigma_y$\".\n",
      "Labeling this model $M_2$, we can write:\n",
      "\n",
      "$$\n",
      "y_{M_2}(x;\\theta) = \\theta_0 + \\theta_1 x + \\theta_2 x^2\\\\\n",
      "y \\sim \\mathcal{N}(y_{M_2}, \\sigma_y^2)\n",
      "$$\n",
      "\n",
      "There are three tunable parameters here, again represented by the vector $\\theta$.\n",
      "\n",
      "- **Model fitting**, in this case, is the process of finding constraints on the values of the parameters $\\theta$ within each model.\n",
      "That is, it allows you to make statements such as, \"assuming $M_1$ is true, this $\\theta$ gives the best-fit line\" or \"assuming $M_2$ is true, this vector $\\theta$ gives the best-fit curve.\"\n",
      "\n",
      "- **Model selection**, on the other hand, is not concerned with the parameters themselves, but with the question of which model as a whole best describes the data.\n",
      "That is, it allows you go say, \"for my data, a line ($M_1$) provides a better fit than a quadratic curve ($M_2$)\".\n",
      "\n",
      "Let's take a look at a concrete example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model Selection: Linear or Quadratic?\n",
      "\n",
      "Discussing statistical concepts in the abstract is rarely helpful, and so we'll start with some data about which we can ask concrete modeling questions.\n",
      "\n",
      "Consider the following data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import numpy as np\n",
      "#\n",
      "#def generate_data(N, rseed=1):\n",
      "#    rng = np.random.RandomState(rseed)\n",
      "#    x = rng.rand(N)\n",
      "#    sigma_y = 0.1 * np.ones(N)\n",
      "#    y = x - 0.2 + sigma_y * rng.randn(N)\n",
      "#    return np.vstack([x, y, sigma_y]).round(2)\n",
      "#\n",
      "#data = generate_data(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "data = np.array([[ 0.42,  0.72,  0.  ,  0.3 ,  0.15,  0.09,  0.19,  0.35,  0.4 ,  0.54,\n",
      "                   0.42,  0.69,  0.2 ,  0.88,  0.03,  0.67,  0.42,  0.56,  0.14,  0.2  ],\n",
      "                 [ 0.33,  0.41, -0.22,  0.01, -0.05, -0.05, -0.12,  0.26,  0.29,  0.39, \n",
      "                   0.31,  0.42, -0.01,  0.58, -0.2 ,  0.52,  0.15,  0.32, -0.13, -0.09 ],\n",
      "                 [ 0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,\n",
      "                   0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1 ,  0.1  ]])\n",
      "x, y, sigma_y = data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get an idea of what we're looking at, let's quickly visualize the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "\n",
      "x, y, sigma_y = data\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
      "ax.set(xlabel='x', ylabel='y', title='input data');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our central model selection question will be this: **what better fits this data: a linear or quadratic curve**?\n",
      "These are the two models, $M_1$ and $M_2$, which we defined above.\n",
      "\n",
      "Let's create a function to compute these models given some data and parameters; for convenience, we'll make a very general polynomial model function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def polynomial_fit(theta, x):\n",
      "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
      "    return sum(t * x ** n for (n, t) in enumerate(theta))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the ``theta`` variable is of length 2, this corresponds to the linear model ($M_1$). If the ``theta`` variable is length 3, this corresponds to the quadratic model ($M_2$).\n",
      "\n",
      "As detailed in my previous posts, both the frequentist and Bayesian approaches to model fitting often revolve around the *likelihood*, and a standard frequentist method to determine the best-fit within each model is maximization of this likelihood.\n",
      "Here is a function which computes the log-likelihood for the two models:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "\n",
      "def logL(theta, model=polynomial_fit, data=data):\n",
      "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
      "    x, y, sigma_y = data\n",
      "    y_fit = model(theta, x)\n",
      "    return sum(stats.norm.logpdf(*args)\n",
      "               for args in zip(y, y_fit, sigma_y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Though there are efficient closed-form ways of maximizing this, we'll use a direct optimization approach here for clarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize\n",
      "\n",
      "neg_logL = lambda *args: -logL(*args)\n",
      "theta1 = optimize.fmin(neg_logL, [0, 0], disp=False)\n",
      "theta2 = optimize.fmin(neg_logL, [0, 0, 0], disp=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now we can visually compare the degree-1 and degree-2 models:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xfit = np.linspace(0, 1, 1000)\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
      "ax.plot(xfit, polynomial_fit(theta1, xfit), label='best linear model')\n",
      "ax.plot(xfit, polynomial_fit(theta2, xfit), label='best quadratic model')\n",
      "ax.legend(loc='best', fontsize=14)\n",
      "ax.set(xlabel='x', ylabel='y', title='data');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The crux of the question is this: **how can we quantitatively decide which model is a better fit to the data?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Fundamentals of Frequentist & Bayesian Model Selection\n",
      "\n",
      "Recall that the fundamental difference between the frequentist and Bayesian approaches is the [**definition of probability**](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/).\n",
      "\n",
      "Frequentists consider **probabilities as frequencies**: that is, a probability is only meaningful in the context of repeated experiments (even if those repetitions are merely hypothetical).\n",
      "This means, for example, that in the frequentist approach:\n",
      "\n",
      "- *observed data* (and any quantities derived from them) are considered to be random variables: if you make the observations again under similar circumstances, the data may be different, and the details depend on the generating distribution.\n",
      "- *model parameters* (those things that help define the generating distribution) are considered fixed: they aren't subject to a probability distribution; they just *are*.\n",
      "\n",
      "On the other hand, Bayesians consider **probabilities as degrees-of-belief**: that is, a probability is a way of quantifying our certainty about a particular statement.\n",
      "This means, for example, that in the Bayesian approach:\n",
      "\n",
      "- *observed data* are not directly considered as random variables; they just *are*.\n",
      "- *model parameters* are uncertain quantities and thus subject to probabilistic description.\n",
      "\n",
      "This difference in philosophy has real practical consequences, as shown in my previous posts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we continue, a quick note on notation.\n",
      "For the below discussion, it is important to be able to denote probabilities concisely.\n",
      "We'll generally be writing conditional probabilities of the form $P(A~|~B)$, which can be read \"the probability of A given B\".\n",
      "Additionally, I'll be using the following shorthands:\n",
      "\n",
      "- $D$ represents observed data\n",
      "- $M$, $M_1$, $M_2$, etc. represent a model\n",
      "- $\\theta$ represents a set of model parameters\n",
      "\n",
      "With this in mind, we'll be writing statements like $P(D~|~\\theta,M)$, which should be read \"the probability of seeing the data, given the parameters $\\theta$ with model $M$.\n",
      "I'm playing a bit fast-and-loose with boolean vs continuous probabilities, but the meaning should be clear from the context."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model Fitting\n",
      "\n",
      "As discussed in previous posts, this difference in the definition of probability, and resulting difference in what quantities are subject to probabilistic description, has actual practical consequences.\n",
      "In particular, when evaluating model parameters, the frequentist and Bayesian approaches deal with different quantities:\n",
      "\n",
      "- frequentists look at the *likelihood*: $P(D~|~\\theta, M)$\n",
      "- Bayesians look at the *posterior*: $P(\\theta~|~D, M)$\n",
      "\n",
      "Note the main distinction: frequentists operate on a **probability of the data**, while Bayesians operate on a **probability of the model**, in line with their respective considerations about the applicability of probability.\n",
      "\n",
      "#### Model Fitting: Frequentist Approach\n",
      "\n",
      "For model fitting, frequentists have a clear advantage: the likelihood is something we can compute directly from the model \u2013 after all, a model is nothing more than a formula to compute the likelihood.\n",
      "By optimizing this equation directly, as we saw above, you can arrive at an estimated best-fit model.\n",
      "\n",
      "#### Model Fitting: Bayesian Approach\n",
      "\n",
      "For model fitting, Bayesians have a more difficult task.\n",
      "To find an expression for the posterior, they can use Bayes' theroem:\n",
      "\n",
      "$$\n",
      "P(\\theta~|~D,M) = \\frac{P(D~|~\\theta,M) \\cdot P(\\theta~|~M)}{P(D~|~M)}\n",
      "$$\n",
      "\n",
      "We see that the posterior is proportional to the likelihood, and the constant of proportionality involves ratio of $P(\\theta~|~M)$ and $P(D~|~M)$.\n",
      "$P(\\theta~|~M)$ here is the *prior* and quantifies our belief/uncertainty about the parameters $\\theta$ without reference to the data.\n",
      "$P(D~|~M)$ is the *model evidence*, and in this context amounts to no more than a normalization term.\n",
      "\n",
      "For more discussion of model fitting in the frequentist and Bayesian contexts, see the previous posts linked above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model Selection\n",
      "\n",
      "Similarly, when comparing two models $M_1$ and $M_2$, the frequentist and Bayesian approaches look at different quantities:\n",
      "\n",
      "- frequentists compare the *model likelihood*, $P(D~|~M_1)$ and $P(D~|~M_2)$\n",
      "- Bayesians compare the *model posterior*, $P(M_1~|~D)$ and $P(M_2~|~D)$\n",
      "\n",
      "Notice here that the parameter values no longer appear; this is an important point.\n",
      "We're not out to compare how well *particular fits* of the two models describe data; we're out to compare how well *the models themselves* describe the data.\n",
      "Note that unlike the parameter likelihood $P(D~|~\\theta, M)$ above, neither quantity here is directly computable from the model and data, and so we must figure out how to re-express the desired quantity in terms we can compute."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Model Selection: Bayesian Approach\n",
      "\n",
      "For model selection, in many ways Bayesians have the advantage, at least theoretically.\n",
      "Through a combination of Bayes Theorem and probability axioms, you can re-express the model posterior $P(M~|~D)$ in terms of computable quantities and priors:\n",
      "\n",
      "First, using Bayes' Theorem:\n",
      "\n",
      "$$\n",
      "P(M~|~D) = P(D~|~M)\\frac{P(M)}{P(D)}\n",
      "$$\n",
      "\n",
      "Using the definition of conditional probability, the first term can be expressed as an integral over the parameter space of the likelihood:\n",
      "\n",
      "$$\n",
      "P(D~|~M) = \\int_\\Omega P(D~|~\\theta, M) P(\\theta~|~M) d\\theta\n",
      "$$\n",
      "\n",
      "Notice that this integral is over the exact quantity optimized in the case of Bayesian model *fitting*.\n",
      "\n",
      "The remaining terms are priors, the most problematic of which is $P(D)$ \u2013 the prior probability of seeing your data *without reference to the model*.\n",
      "I'm not sure that $P(D)$ could ever be actually computed, but fortunately it can be canceled by computing the *odds ratio* between two models:\n",
      "\n",
      "$$\n",
      "O_{21} \\equiv \\frac{P(M_2~|~D)}{P(M_1~|~D)} = \\frac{P(D~|~M_2)}{P(D~|~M_1)}\\frac{P(M_2)}{P(M_1)}\n",
      "$$\n",
      "\n",
      "We now have a means of comparing two models via computable quantities: an integral over the likelihood for each, and a prior odds for each.\n",
      "Often the prior odds is assumed to be near unity, leaving only the well-defined (but often computationally intensive) integral over likelihood for each model.\n",
      "More on this below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Model Selection: Frequentist Approach\n",
      "\n",
      "For model selection, frequentists are working with the quantity $P(D~|~M)$.\n",
      "Notice that unlike Bayesians, frequentists *cannot* express this as an integral over parameter space, because the notion of a probability distribution over model parameters does not make sense in the frequentist context.\n",
      "But recall that frequentists can make probabilistic statements about data or quantities derived from them: with this in mind, we can make progress by **computing some *statistic* from the data for which the distribution is known**.\n",
      "The difficulty is that which statistic is most useful depends highly on the precise model and data being used, and so practicing frequentist statistics requires a breadth of background knowledge about the assumptions made by various approaches.\n",
      "\n",
      "For example, one commonly-seen distribution for data-derived statistis is the [$\\chi^2$ (chi-squared) distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution).\n",
      "The $\\chi^2$ distribution with $K$ degrees of freedom describes the distribution of a sum of squares of $K$ independent normally-distributed variables.\n",
      "We can use Python tools to quickly visualize this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "v = np.linspace(0, 8, 1000)\n",
      "for k in range(1, 7):\n",
      "    plt.plot(v, stats.chi2.pdf(v, k),\n",
      "             label=\"k = {0}\".format(k))\n",
      "plt.legend(ncol=2)\n",
      "plt.gca().set(title='$\\chi^2$ distribution',\n",
      "              xlabel='x', ylabel='p(x)', ylim=(0, 0.5));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The key observation is this: if we can somehow *transform our data* into a single statistic $S(D;\\theta,M)$ which we expect behaves like the sum of squares of normally-distributed values, then we can analyze the likelihood of this statistic in terms of this $\\chi^2$ distribution, and use this as a proxy for the model likelihood $P(D~|~M)$.\n",
      "\n",
      "In the case of our linear and quadratic model likelihoods $M_1$ and $M_2$ above, the models are built on the explicit expectation that for the correct model, the observed data $y$ will be normally distributed about the model value $y_M$ with a standard deviation $\\sigma_y$.\n",
      "Thus the sum of squares of normalized residuals about the best-fit model should follow the $\\chi^2$ distribution, and so we construct our $\\chi^2$ statistic:\n",
      "\n",
      "$$\n",
      "\\chi^2(D;\\theta,M) = \\sum_n\\left[\\frac{y_n - y_M(x_n;\\theta)}{\\sigma_{y,n}}\\right]^2\n",
      "$$\n",
      "\n",
      "Coincidentally (or is it?), this statistic is proportional to the negative log likelihood, up to a constant offset.\n",
      "\n",
      "Notice what we've done here: we've replaced an *uncomputable* quantity $P(D~|~M)$ with a *computable* quantity $P(S~|~M_S)$, where $S = \\chi^2$ is a particular transformation of the data for which (under our model assumptions) we can compute the probability distribution.\n",
      "So rather than asking \"how likely are we to see the data $D$ under the model $M$\", we can ask \"how likely are we to see the statistic $S$ under the model $M_S$\", and the two likelihoods are essentially equivalent *as long as our assumptions hold*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Theory to Practice: Linear or Quadratic?\n",
      "\n",
      "Let's use the ideas developed above to address the above model selection problem in both a frequentist and Bayesian context.\n",
      "First, though, let's take a look at what **not** to do."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Can We Directly Compare Likelihoods?\n",
      "\n",
      "One common mistake is to assume that we can select between models via *the value of the maximum likelihood*.\n",
      "While this works in some special cases, it is not generally applicable.\n",
      "Let's take a look at the maximum log-likelihood value for each of our fits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"linear model:    logL =\", logL(theta1))\n",
      "print(\"quadratic model: logL =\", logL(theta2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The quadratic model yields a higher log-likelihood, but this **does not** necessarily mean it is the better model!\n",
      "\n",
      "The problem is that the quadratic model has more degrees of freedom than the linear model, and thus will have an equal or larger maximum likelihood for **any data**.\n",
      "This also extends to higher-order polynomial models: using this maximum likelihood comparison, a cubic model will always outperform a quadratic model, a quartic model will outperform a cubic, etc.\n",
      "Let's take a look at this by plotting the maximum log-likelihood as a function of polynomial degree:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def best_theta(degree, model=polynomial_fit, data=data):\n",
      "    theta_0 = (degree + 1) * [0]\n",
      "    neg_logL = lambda theta: -logL(theta, model, data)\n",
      "    return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)\n",
      "    \n",
      "degrees = np.arange(1, 10)\n",
      "thetas = [best_theta(d) for d in degrees]\n",
      "logL_max = [logL(theta) for theta in thetas]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
      "ax[0].plot(degrees, logL_max)\n",
      "ax[0].set(xlabel='degree', ylabel='log(Lmax)')\n",
      "\n",
      "ax[1].errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
      "ylim = ax[1].get_ylim()\n",
      "for (degree, theta) in zip(degrees, thetas):\n",
      "    if degree not in [1, 2, 9]: continue\n",
      "    ax[1].plot(xfit, polynomial_fit(theta, xfit),\n",
      "               label='degree={0}'.format(degree))\n",
      "ax[1].set(ylim=ylim, xlabel='x', ylabel='y')\n",
      "ax[1].legend(fontsize=14, loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected, we see in the left panel that the maximum likelihood *value* increases as we increase the degree of the polynomial, but looking at the right panel, we would not say that a ninth-order polynomial is a *better model*.\n",
      "The more complicated models achieve a high maximum likelihood by **over-fitting** the data, responding to the individual noise in the points rather than the underlying trend.\n",
      "\n",
      "In this way, the model selection problem can simply be cast as simply a process of preventing overfitting.\n",
      "We will come back to this view below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Frequentist Model Selection: $\\chi^2$\n",
      "\n",
      "Let's go beyond likelihood comparisons, and take a look at what the $\\chi^2$ statistic says about these models.\n",
      "Recall that we expect for the true model, the sum of normalized residuals will be drawn from a $\\chi^2$ distribuion with a certain number of degrees of freedom related to the number of data points.\n",
      "For $N$ data points fit by a $K$-parameter linear model, the degrees of freedom are usually given by $dof = N - K$, though there are some caveats to this (See [The Dos and Don'ts of Reduced Chi-Squared](http://arxiv.org/abs/1012.3754) for an enlightening discussion).\n",
      "\n",
      "Let's define some functions to compute the $\\chi^2$ and the number of degrees of freedom, and evaluate the likelihood of each model with the $\\chi^2$ distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_chi2(degree, data=data):\n",
      "    x, y, sigma_y = data\n",
      "    theta = best_theta(degree, data=data)\n",
      "    resid = (y - polynomial_fit(theta, x)) / sigma_y\n",
      "    return np.sum(resid ** 2)\n",
      "\n",
      "def compute_dof(degree, data=data):\n",
      "    return data.shape[1] - (degree + 1)\n",
      "\n",
      "def chi2_likelihood(degree, data=data):\n",
      "    chi2 = compute_chi2(degree, data)\n",
      "    dof = compute_dof(degree, data)\n",
      "    return stats.chi2(dof).pdf(chi2)\n",
      "\n",
      "print(\"chi2 likelihood\")\n",
      "print(\"- linear model:    \", chi2_likelihood(1))\n",
      "print(\"- quadratic model: \", chi2_likelihood(2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What we have found is that the likelihood of the observed residuals under the linear model ($M_1$) is slightly larger than the likelihood of the observed residuals under the quadratic model ($M_2$).\n",
      "\n",
      "To understand what this is saying, it is helpful to visualize these distributions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots()\n",
      "for degree, color in zip([1, 2], ['blue', 'green']):\n",
      "    v = np.linspace(0, 40, 1000)\n",
      "    chi2_dist = stats.chi2(compute_dof(degree)).pdf(v)\n",
      "    chi2_val = compute_chi2(degree)\n",
      "    chi2_like = chi2_likelihood(degree)\n",
      "    ax.fill(v, chi2_dist, alpha=0.3, color=color,\n",
      "            label='Model {0} (degree = {0})'.format(degree))\n",
      "    ax.vlines(chi2_val, 0, chi2_like, color=color, alpha=0.6)\n",
      "    ax.hlines(chi2_like, 0, chi2_val, color=color, alpha=0.6)\n",
      "    ax.set(ylabel='L(chi-square)')\n",
      "ax.set_xlabel('chi-square')\n",
      "ax.legend(fontsize=14);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see visually here how this procedure corrects for model complexity: even though the $\\chi^2$ *value* for the quadratic model is lower (shown by the vertical lines), the differing degrees of freedom mean the *likelihood* of seeing this value is higher (shown by the horizontal lines), meaning that the degree=1 linear model is favored."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Significance of the Comparison\n",
      "\n",
      "But how much should we trust this conclusion in favor of the linear model?\n",
      "In other words, how do we quantify the **significance** of this difference in $\\chi^2$ likelihoods?\n",
      "\n",
      "We can make progress by realizing that in the frequentist context *all data-derived quantities* can be consistered probabilistically, and this includes the difference in $\\chi^2$ values from two models!\n",
      "For this particular case, the difference of $\\chi^2$ statistics here also follows a $\\chi^2$ distribution, with 1 degree of freedom. This is due to the fact that the models are *nested* \u2013 that is, the linear model is a specialization of the quadratic model (for some background, look up the [Likelihood Ratio Test](https://en.wikipedia.org/wiki/Likelihood-ratio_test)).\n",
      "\n",
      "For this, we can treat the linear model as the *null hypothesis*, and ask if there is sufficient evidence to justify the more complicated quadratic model.\n",
      "Just as above, we can plot the $\\chi^2$ difference along with the expected distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chi2_diff = compute_chi2(1) - compute_chi2(2)\n",
      "\n",
      "v = np.linspace(0, 5, 1000)\n",
      "chi2_dist = stats.chi2(1).pdf(v)\n",
      "p_value = 1 - stats.chi2(1).cdf(chi2_diff)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.fill_between(v, 0, chi2_dist, alpha=0.3)\n",
      "ax.fill_between(v, 0, chi2_dist * (v > chi2_diff), alpha=0.5)\n",
      "ax.axvline(chi2_diff)\n",
      "ax.set(ylim=(0, 1), xlabel=\"$\\chi^2$ difference\", ylabel=\"probability\");\n",
      "ax.text(4.9, 0.95, \"p = {0:.2f}\".format(p_value),\n",
      "        ha='right', va='top', size=14);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see where this $\\chi^2$ difference lies on its expected distribution, under the null hypothesis that the linear model is the true model.\n",
      "The area of the distribution to the right of the observed value is known as the [*p value*](https://en.wikipedia.org/wiki/P-value): the probability that the test would favor the quadratic distribution by chance.\n",
      "According to this $p$-value, there is a 17% probability that our results would favor the quadratic model by chance.\n",
      "We should interpret this to say that the results are *not inconsistent with the null hypothesis*: that is, our data does not support the quadratic model enough to conclusively reject the linear model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Other Frequentist Approaches\n",
      "\n",
      "Recall that a primary purpose of using the $\\chi^2$ statistic, above, was to prevent mistaken selection of very flexible models which *over-fit* the data.\n",
      "Other qualitatively different approaches exist to limit this overfitting.\n",
      "Two important ones are:\n",
      "\n",
      "- [The Aikake Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) is an information-theoretic approach which penalizes the maximum likelihood to account for added model complexity. It makes rather stringent assumptions about the form of the likelihood, and so cannot be universally applied.\n",
      "- [Cross-Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics) is a sampling method in which the model fit and evaluation take place on disjoint randomized subsets of the data, which acts to penalize over-fitting. Cross-validation is more computationally intensive than other frequentist approaches, but has the advantage that it relies on very few assumptions about the data and model and so is applicable to a broad range of models.\n",
      "\n",
      "We will not demonstrate these here, but the approaches are relatively straightforward."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Bayesian Model Selection: the Odds Ratio\n",
      "\n",
      "The Bayesian approach proceeds very differently.\n",
      "Recall that the Bayesian model involves computing the *odds ratio* between two models:\n",
      "\n",
      "$$\n",
      "O_{21} = \\frac{P(M_2~|~D)}{P(M_1~|~D)} = \\frac{P(D~|~M_2)}{P(D~|~M_1)}\\frac{P(M_2)}{P(M_1)}\n",
      "$$\n",
      "\n",
      "Here the ratio $P(M_2) / P(M_1)$ is the *prior odds ratio*, and is often assumed to be equal to 1.\n",
      "The ratio $P(D~|~M_2) / P(D~|~M_1)$ is the *Bayes factor*, and is the key to Bayesian model selection.\n",
      "\n",
      "The Bayes factor is usually computed by somehow evaluating the integral over the parameter likelihood:\n",
      "\n",
      "$$\n",
      "P(D~|~M) = \\int_\\Omega P(D~|~\\theta, M) P(\\theta~|~M) d\\theta\n",
      "$$\n",
      "\n",
      "This integral is over the entire parameter space of the model, and thus can be extremely computationally intensive, especially as the dimension of the model grows beyond a few.\n",
      "For the 2-dimensional and 3-dimensional models we are considering here, however, this can be done directly via numerical integration.\n",
      "\n",
      "We'll start, though, by using an MCMC to draw samples from the posterior in order to solve the *model fitting* problem.\n",
      "We will use the [emcee](http://dan.iel.fm/emcee) package, which requires us to first define functions which compute the prior, likelihood, and posterior under each model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_prior(theta):\n",
      "    # size of theta determines the model.\n",
      "    # flat prior over a large range\n",
      "    if np.any(abs(theta) > 10):\n",
      "        return -np.inf  # log(0)\n",
      "    else:\n",
      "        return 200 ** -len(theta)\n",
      "\n",
      "def log_likelihood(theta, data=data):\n",
      "    x, y, sigma_y = data\n",
      "    yM = polynomial_fit(theta, x)\n",
      "    return -0.5 * np.sum(np.log(2 * np.pi * sigma_y ** 2)\n",
      "                         + (y - yM) ** 2 / sigma_y ** 2)\n",
      "\n",
      "def log_posterior(theta, data=data):\n",
      "    theta = np.asarray(theta)\n",
      "    return log_prior(theta) + log_likelihood(theta, data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we draw samples from the posterior using MCMC:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import emcee\n",
      "\n",
      "def polymodel_mcmc(degree, data=data,\n",
      "                   log_posterior=log_posterior,\n",
      "                   nwalkers=50, nburn=1000, nsteps=2000):\n",
      "    ndim = degree + 1  # this determines the model\n",
      "    rng = np.random.RandomState(0)\n",
      "    starting_guesses = rng.randn(nwalkers, ndim)\n",
      "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[data])\n",
      "    sampler.run_mcmc(starting_guesses, nsteps)\n",
      "    trace = sampler.chain[:, nburn:, :].reshape(-1, ndim)\n",
      "    return trace\n",
      "\n",
      "trace_2D = polymodel_mcmc(1)\n",
      "trace_3D = polymodel_mcmc(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To visualize the posterior samples, I like to use seaborn's ``jointplot`` (for 2D samples) or ``PairGrid`` (for N-D samples):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "columns = [r'$\\theta_{0}$'.format(i) for i in range(3)]\n",
      "df_2D = pd.DataFrame(trace_2D, columns=columns[:2])\n",
      "\n",
      "with sns.axes_style('ticks'):\n",
      "    jointplot = sns.jointplot(r'$\\theta_0$', r'$\\theta_1$',\n",
      "                              data=df_2D, kind=\"hex\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_3D = pd.DataFrame(trace_3D, columns=columns[:3])\n",
      "\n",
      "# get the colormap from the joint plot above\n",
      "cmap = jointplot.ax_joint.collections[0].get_cmap()\n",
      "\n",
      "with sns.axes_style('ticks'):\n",
      "    grid = sns.PairGrid(df_3D)\n",
      "    grid.map_diag(plt.hist, bins=30, alpha=0.5)\n",
      "    grid.map_offdiag(plt.hexbin, gridsize=50, linewidths=0, cmap=cmap)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These samples give us a good idea of what the posterior looks like, but we still must integrate this posterior to find the Bayes factor.\n",
      "\n",
      "For these lower-dimensional models, we'll do direct numerical integration using tools from the ``scipy.integrate`` package to integrate the posterior and compute the odds ratio. \n",
      "The call signature of the multiple integration routines is a bit confusing \u2013 I suggest referring to the [scipy.integrate documentation](http://docs.scipy.org/doc/scipy/reference/tutorial/integrate.html) to read about the inputs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import integrate\n",
      "\n",
      "def integrate_posterior_2D(log_posterior, xlim, ylim, data=data):\n",
      "    func = lambda theta1, theta0: np.exp(log_posterior([theta0, theta1], data))\n",
      "    return integrate.dblquad(func, xlim[0], xlim[1],\n",
      "                             lambda x: ylim[0], lambda x: ylim[1])\n",
      "\n",
      "def integrate_posterior_3D(log_posterior, xlim, ylim, zlim, data=data):\n",
      "    func = lambda theta2, theta1, theta0: np.exp(log_posterior([theta0, theta1, theta2], data))\n",
      "    return integrate.tplquad(func, xlim[0], xlim[1],\n",
      "                             lambda x: ylim[0], lambda x: ylim[1],\n",
      "                             lambda x, y: zlim[0], lambda x, y: zlim[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The tricky part of the integration is choosing the integration limits correctly; fortunately we can use the MCMC traces to find appropriate values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xlim, ylim = zip(trace_2D.min(0), trace_2D.max(0))\n",
      "Z1, err_Z1 = integrate_posterior_2D(log_posterior, xlim, ylim)\n",
      "print(Z1, \"+/-\", err_Z1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xlim, ylim, zlim = zip(trace_3D.min(0), trace_3D.max(0))\n",
      "Z2, err_Z2 = integrate_posterior_3D(log_posterior, xlim, ylim, zlim)\n",
      "print(Z2, \"+/-\", err_Z2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z2 / Z1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Bayes factor favors the quadratic model, but only slightly (an odds of about 7 to 3).\n",
      "In fact, this value for the Bayes factor ranks as \"not worth a mere mention\" according to the scale proposed by [Kass & Raferty (1995)](https://www.andrew.cmu.edu/user/kk3n/simplicity/KassRaftery1995.pdf) an influential paper on the subject.\n",
      "Notice that this interpretation is very similar to what we found with the frequentist approach above, which favors the quadratic model, but has too large a $p$-value to support discarding the simpler linear model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Other Bayesian approaches\n",
      "\n",
      "While direct numerical integration of the posterior works for low-dimensional models, computing Bayes factors for higher-dimensional models requires either a more sophisticated method, or an approximation of the integral.\n",
      "Here is an incomplete list of approaches you might turn to:\n",
      "\n",
      "- [Nested Sampling](https://en.wikipedia.org/wiki/Nested_sampling_algorithm) is a sampling-based algorithm like MCMC, which is specially designed to compute Bayes factors.\n",
      "- [Reversible Jump MCMC](https://en.wikipedia.org/wiki/Reversible-jump_Markov_chain_Monte_Carlo) (also see *bridge sampling*) can be used to sample multiple models in the same MCMC chain, so that the Bayes factor can be estimated directly from the joint chain (see, e.g. [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.399.8889&rep=rep1&type=pdf))\n",
      "- [the Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion) quickly approximates the Bayes factor using rather strong assumptions about the form of the posterior. It is qualitatively very similar to the Aikake Information Criterion (AIC), mentioned above.\n",
      "\n",
      "Again, I will not demonstrate any of these techniques here, but my hope is that this post has given you the background to understand them from other available references."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Discussion\n",
      "\n",
      "blah blah blah"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}