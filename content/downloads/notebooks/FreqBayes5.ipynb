{
 "metadata": {
  "name": "",
  "signature": "sha256:d77b3233aecf1718090a368e5c7fbaf9cee0f6665b93256b114ae9d02a3bccf7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Frequentism and Bayesianism V: Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Frequentist\n",
      "\n",
      "- Likelihood ratio\n",
      "- Reduced $\\chi^2$\n",
      "- information criteria: AIC\n",
      "- Cross Validation\n",
      "\n",
      "## Bayesian\n",
      "- Bayes Factor\n",
      "- KDE estimate?\n",
      "- BIC\n",
      "\n",
      "## Problem setting\n",
      "- Is my data linear or quadratic in time?\n",
      "- More general: polynomial of degree $N$\n",
      "\n",
      "## Some References\n",
      "\n",
      "- [Dos and don\u2019ts of reduced chi-squared](http://arxiv.org/pdf/1012.3754v1.pdf)\n",
      "- http://www.stat.washington.edu/raftery/Research/PDF/kass1995.pdf\n",
      "- http://arxiv.org/abs/0907.5123\n",
      "- http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9574.2011.00515.x/abstract\n",
      "- https://projecteuclid.org/download/pdfview_1/euclid.ba/1346158782"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Intro: Line or Curve?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "import seaborn as sns; sns.set()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "def PolynomialRegression(degree, *args, **kwargs):\n",
      "    return make_pipeline(PolynomialFeatures(degree),\n",
      "                         LinearRegression(*args, **kwargs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: hide this\n",
      "\n",
      "def generate_data(N, rseed=42):\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    x = rng.rand(N)\n",
      "    dy = 0.2\n",
      "    y = 2 * x ** 2 + 0.5 * x + dy * rng.randn(N)\n",
      "    xfit = np.linspace(0, 1)\n",
      "    return x, y, dy\n",
      "\n",
      "x, y, dy = generate_data(20)\n",
      "data = np.vstack([x, y, np.full_like(x, dy)]).round(3).T\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, y, dy = data.T\n",
      "plt.errorbar(x, y, dy, fmt='ok', ecolor='gray');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize\n",
      "\n",
      "def model(theta, x):\n",
      "    return sum(t * x ** n for n, t in enumerate(theta))\n",
      "\n",
      "def minus_logL(theta, x, y, dy):\n",
      "    return 0.5 * np.sum((y - model(theta, x)) ** 2 / dy ** 2)\n",
      "\n",
      "def compute_fit(cost, degree, x, y, dy):\n",
      "    theta_0 = [0 for i in range(degree + 1)]\n",
      "    return optimize.fmin_bfgs(cost, theta_0, args=(x, y, dy), disp=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_fit(x, y, dy, degree, ax=None):\n",
      "    if ax is None:\n",
      "        ax = plt.gca()\n",
      "    theta_hat = compute_fit(minus_logL, degree, x, y, dy)\n",
      "    xfit = np.linspace(0, 1)\n",
      "    yfit = model(theta_hat, xfit)\n",
      "    \n",
      "    ax.errorbar(x, y, dy, fmt='ok', ecolor='gray');\n",
      "    ax.plot(xfit, yfit)\n",
      "    ax.set_title('degree = {0}'.format(degree))\n",
      "    ax.text(0.05, 0.95, \"logL = {0:.3g}\".format(-minus_logL(theta_hat, x, y, dy)),\n",
      "            transform=ax.transAxes, size=14, va='top')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
      "for i, degree in enumerate([1, 2]):\n",
      "    plot_fit(x, y, dy, degree, ax[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
      "for i, degree in enumerate([3, 10]):\n",
      "    plot_fit(x, y, dy, degree, ax[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Frequentist Approaches\n",
      "\n",
      "- \"chi-square\" vs \"reduced chi-square\"\n",
      "- Information Criteria\n",
      "- Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Reduced Chi-square"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chi2(theta, x, y, dy):\n",
      "    return np.sum((y - model(theta, x)) ** 2 / dy ** 2)\n",
      "\n",
      "def compute_chi2(degree, x, y, dy):\n",
      "    theta_hat = compute_fit(chi2, degree, x, y, dy)\n",
      "    return chi2(theta_hat, x, y, dy)\n",
      "\n",
      "def compute_reduced_chi2(degree, x, y, dy):\n",
      "    dof = len(x) - (degree + 1)\n",
      "    return compute_chi2(degree, x, y, dy) / dof\n",
      "\n",
      "\n",
      "degrees = range(1, 11)\n",
      "chi2_results = [compute_chi2(degree, x, y, dy) / len(x)\n",
      "                for degree in degrees]\n",
      "reduced_chi2_results = [compute_reduced_chi2(degree, x, y, dy)\n",
      "                        for degree in degrees]\n",
      "\n",
      "plt.plot(degrees, chi2_results, label='chi2 (scaled)')\n",
      "plt.plot(degrees, reduced_chi2_results, label='reduced chi2')\n",
      "plt.xlabel('degree')\n",
      "plt.legend();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Aikake Information Criterion\n",
      "\n",
      "$$\n",
      "AIC = -2\\log L + 2k\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_logL(degree, x, y, dy):\n",
      "    theta_hat = compute_fit(minus_logL, degree, x, y, dy)\n",
      "    return -0.5 * np.sum((y - model(theta_hat, x)) ** 2 / dy ** 2)\n",
      "\n",
      "def compute_AIC(degree, x, y, dy):\n",
      "    return 2 * (degree + 1) - 2 * compute_logL(degree, x, y, dy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logL_results = [-compute_logL(degree, x, y, dy)\n",
      "                for degree in degrees]\n",
      "AIC_results = [compute_AIC(degree, x, y, dy)\n",
      "               for degree in degrees]\n",
      "\n",
      "plt.plot(degrees, logL_results,\n",
      "         label='-log(L)')\n",
      "plt.plot(degrees, AIC_results,\n",
      "         label='AIC')\n",
      "plt.xlabel('degree')\n",
      "plt.legend();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross-Validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import LeaveOneOut\n",
      "list(LeaveOneOut(5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_CV_score(degree, x, y, dy):\n",
      "    def chi2_CV():\n",
      "        for (train, test) in LeaveOneOut(len(x)):\n",
      "            theta_hat = compute_fit(chi2, degree,\n",
      "                                    x[train], y[train], dy[train])\n",
      "            yield chi2(theta_hat, x[test], y[test], dy[test])\n",
      "    return sum(chi2_CV())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CV_results = [compute_CV_score(degree, x, y, dy)\n",
      "              for degree in degrees]\n",
      "chi2_results = [compute_chi2(degree, x, y, dy)\n",
      "                for degree in degrees]\n",
      "\n",
      "plt.plot(degrees, chi2_results,\n",
      "         label='chi2')\n",
      "plt.plot(degrees, CV_results,\n",
      "         label='chi2 CV')\n",
      "plt.xlabel('degree')\n",
      "plt.legend();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bayesian Model Selection\n",
      "\n",
      "- Odds Ratio http://stronginference.com/bayes-factors-pymc.html\n",
      "\n",
      "- Harmonic Mean: https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Fitting the Models\n",
      "\n",
      "We'll use [emcee](http://dan.iel.fm/emcee/)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_prior(theta):\n",
      "    if np.any(abs(theta) > 100):\n",
      "        return -np.inf  # log(0)\n",
      "    else:\n",
      "        return 200 ** -len(theta)\n",
      "    \n",
      "def y_model(theta, x):\n",
      "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
      "\n",
      "def log_likelihood(theta, x, y, dy):\n",
      "    yM = y_model(theta, x)\n",
      "    return -0.5 * np.sum(np.log(2 * np.pi * dy ** 2) + (y - yM) ** 2 / dy ** 2)\n",
      "\n",
      "def log_posterior(theta, x, y, dy):\n",
      "    theta = np.asarray(theta)\n",
      "    return log_prior(theta) + log_likelihood(theta, x, y, dy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import emcee\n",
      "\n",
      "def polymodel_mcmc(degree,\n",
      "                   x=x, y=y, dy=dy,\n",
      "                   log_posterior=log_posterior,\n",
      "                   nwalkers=50, nburn=1000, nsteps=2000):\n",
      "    ndim = degree + 1\n",
      "    rng = np.random.RandomState(0)\n",
      "    starting_guesses = rng.randn(nwalkers, ndim)\n",
      "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x, y, dy])\n",
      "    sampler.run_mcmc(starting_guesses, nsteps)\n",
      "    trace = sampler.chain[:, nburn:, :].reshape(-1, ndim)\n",
      "    return trace"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace1 = polymodel_mcmc(1)\n",
      "data1 = pd.DataFrame(trace1, columns=['intercept', 'slope'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with sns.axes_style('ticks'):\n",
      "    jnt = sns.jointplot('intercept', 'slope', data=data1, kind=\"hex\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace2 = polymodel_mcmc(degree=2)\n",
      "data2 = pd.DataFrame(trace2, columns=[r'$\\theta_0$ (intercept)',\n",
      "                                      r'$\\theta_1$ (slope)',\n",
      "                                      r'$\\theta_2$ (quadratic)'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the colormap from the joint plot above\n",
      "cmap = jnt.ax_joint.collections[0].get_cmap()\n",
      "\n",
      "def hexbin(x, y, color, cmap=cmap, **kwargs):\n",
      "    plt.hexbin(x, y, gridsize=50, cmap=cmap, **kwargs)\n",
      "\n",
      "with sns.axes_style('ticks'):\n",
      "    grid = sns.PairGrid(data2)\n",
      "    grid.map_diag(plt.hist, bins=30, alpha=0.5)\n",
      "    grid.map_offdiag(hexbin);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we compute the Bayes factor...\n",
      "\n",
      "(show math)\n",
      "\n",
      "Integrate the result with scipy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import integrate\n",
      "\n",
      "def integrate_posterior(trace, log_posterior, x=x, y=y, dy=dy):\n",
      "    tmin = trace.min(0)\n",
      "    tmax = trace.max(0)\n",
      "    if trace.shape[1] == 1:\n",
      "        func = lambda theta0: log_posterior([theta0], x, y, dy)\n",
      "        Z, dZ = integrate.quad(func, tmin[0], tmax[0])\n",
      "    elif trace.shape[1] == 2:\n",
      "        func = lambda theta1, theta0: np.exp(log_posterior([theta0, theta1], x, y, dy))\n",
      "        Z, dZ = integrate.dblquad(func,\n",
      "                                  tmin[0], tmax[0],\n",
      "                                  lambda x: tmin[1], lambda x: tmax[1])\n",
      "    elif trace.shape[1] == 3:\n",
      "        func = lambda theta2, theta1, theta0: np.exp(log_posterior([theta0, theta1, theta2], x, y, dy))\n",
      "        Z, dZ = integrate.tplquad(func,\n",
      "                                  tmin[0], tmax[0],\n",
      "                                  lambda x: tmin[1], lambda x: tmax[1],\n",
      "                                  lambda x, y: tmin[2], lambda x, y: tmax[2])\n",
      "    else:\n",
      "        raise ValueError(\"only supports 1, 2, or 3 dimensional data\")\n",
      "    \n",
      "    return Z, dZ"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z1, dZ1 = integrate_posterior(trace1, log_posterior)\n",
      "print(\"Z_1 = {0:.3g} +/- {1:.2g}\".format(Z1, dZ1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z2, dZ2 = integrate_posterior(trace2, log_posterior)\n",
      "print(\"Z_2 = {0:.3g} +/- {1:.2g}\".format(Z2, dZ2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Odds Ratio: {0:.0f}\".format(Z2 / Z1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the Bayesian approach favors a degree-2 model over a degree-1 model by odds of over 5000 to 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### degree 1 model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace1 = polymodel_mcmc(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z1_direct = compute_Z_direct(trace1, log_posterior)\n",
      "Z1_KDE = compute_Z_KDE(trace1, log_posterior)\n",
      "Z1_HM = compute_Z_HM(trace1, log_likelihood)\n",
      "\n",
      "print(\"direct:\", Z1_direct)\n",
      "print(\"KDE:   \", Z1_KDE)\n",
      "print(\"HM:    \", Z1_HM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### degree-2 model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace2 = polymodel_mcmc(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z2_direct = compute_Z_direct(trace2, log_posterior)\n",
      "Z2_KDE = compute_Z_KDE(trace2, log_posterior)\n",
      "Z2_HM = compute_Z_HM(trace2, log_likelihood)\n",
      "\n",
      "print(\"direct:\", Z2_direct)\n",
      "print(\"KDE:   \", Z2_KDE)\n",
      "print(\"HM:    \", Z2_HM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### BIC\n",
      "\n",
      "$$\n",
      "BIC = -2\\log L + k\\log N\n",
      "$$\n",
      "\n",
      "$$\n",
      "AIC = -2\\log L + 2k\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_BIC(degree, x, y, dy):\n",
      "    return (degree + 1) * np.log(len(x)) - 2 * compute_logL(degree, x, y, dy)\n",
      "\n",
      "AIC_results = [compute_AIC(degree, x, y, dy)\n",
      "               for degree in degrees]\n",
      "BIC_results = [compute_BIC(degree, x, y, dy)\n",
      "               for degree in degrees]\n",
      "\n",
      "plt.plot(degrees, BIC_results,\n",
      "         label='BIC')\n",
      "plt.plot(degrees, AIC_results,\n",
      "         label='AIC')\n",
      "plt.xlabel('degree')\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other options\n",
      "\n",
      "- Harmonic mean method: it's common, but horrible. You basically should [never use it](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/). However, some have proposed [modified versions](http://projecteuclid.org/euclid.ba/1346158782). PyMC doesn't currently implement this, but for models of constant dimension you can do something [very similar](http://stronginference.com/bayes-factors-pymc.html))\n",
      "- Estimation by thermodynamic tempering (e.g. in [Parallel Tempering Ensemble MCMC](http://dan.iel.fm/emcee/current/user/pt/))\n",
      "- [Nested Sampling](https://en.wikipedia.org/wiki/Nested_sampling_algorithm) or [Reversible Jump MCMC](https://en.wikipedia.org/wiki/Reversible-jump_Markov_chain_Monte_Carlo)\n",
      "- Posterior Predictive Checks (a good resource is [this 70-page review of Bayesian model selection](http://projecteuclid.org/download/pdfview_1/euclid.ssu/1356628931)\n",
      "- http://drsmorey.org/bibtex/upload/Morey:etal:2011a.pdf\n",
      "- Kernel Density Estimation \u2013 I can't find any references, but the idea seems obvious (what's wrong with it?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_Z_HM(trace, log_likelihood, x=x, y=y, dy=dy):\n",
      "    logL = np.array([log_likelihood(theta, x, y, dy) for theta in trace])\n",
      "    return len(logL) / np.sum(np.exp(-logL))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "def compute_Z_KDE(trace, log_posterior, x=x, y=y, dy=dy):\n",
      "    h = 0.01 * np.mean(trace.max(0) - trace.min(0))\n",
      "    kde = KernelDensity(h, rtol=1E-3)\n",
      "    kde.fit(trace1[::10])\n",
      "    logp = kde.score_samples(trace1[::10])\n",
      "    logL = np.array([log_posterior(theta, x, y, dy)\n",
      "                     for theta in trace[::10]])\n",
      "    log_Z2_estimate = logL - logp\n",
      "    return np.exp(np.median(log_Z2_estimate))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}