{
 "metadata": {
  "name": "",
  "signature": "sha256:3d071b80f941905322b433a29232c97043e95f23a4e9361dab28b3e900731c96"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Frequentism and Bayesianism V: Model Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*This notebook originally appeared as a [post](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/) on the blog [Pythonic Perambulations](http://jakevdp.github.io). The content is BSD licensed.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*This post is part of a 4-part series: [Part I](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)  [Part II](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/)  [Part III](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/)  [Part IV](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/)*\n",
      "\n",
      "*See also [Frequentism and Bayesianism: A Python-driven Primer](http://arxiv.org/abs/1411.5018), a peer-reviewed article partially based on this content.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Last year I wrote a series of posts comparing frequentist and Bayesian approaches to various problems:\n",
      "\n",
      "- In [Frequentism and Bayesianism I: a Practical Introduction](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) I gave an introduction to the main philosophical differences between frequentism and Bayesianism, and showed that for many common problems the two methods give basically the same point estimates.\n",
      "- In [Frequentism and Bayesianism II: When Results Differ](http://jakevdp.github.io/blog/2014/06/06/frequentism-and-bayesianism-2-when-results-differ/) I went into a bit more depth on when frequentism and Bayesianism start to diverge, particularly when it comes to the handling of nuisance parameters.\n",
      "- In [Frequentism and Bayesianism III: Confidence, Credibility, and why Frequentism and Science Don't Mix](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/) I talked about the subtle difference between frequentist confidence intervals and Bayesian credible intervals, and argued that in most scientific settings frequentism answers the wrong question.\n",
      "- In [Frequentism and Bayesianism IV: How to be a Bayesian in Python](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/) I compared three Python packages for doing Bayesian analysis via MCMC: [emcee](http://dan.iel.fm/emcee), [pymc](http://pymc-devs.github.io/pymc/), and [pystan](https://pystan.readthedocs.org/en/latest/).\n",
      "\n",
      "One important piece that I've not yet covered is the notion of *model selection*.\n",
      "That is the subject of this post."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Model Fitting vs Model Selection\n",
      "\n",
      "The difference between model fitting and model selection sometimes causes confusion.\n",
      "By model, here, I mean a formula, usually with tunable parameters, that can describe and/or generate data.\n",
      "\n",
      "For example, your model might consist of the statement, \"the observations come from a straight line, with known normal measurement errors\".\n",
      "Mathematically, this says:\n",
      "\n",
      "$$\n",
      "y_{M1}(x;\\theta) = \\theta_0 + \\theta_1 x\\\\\n",
      "y \\sim \\mathcal{N}(y_{M1}, \\sigma_y)\n",
      "$$\n",
      "\n",
      "where the second line indicates that the observed $y$ is normally distributed about the model value.\n",
      "There are two tunable parameters to this model, represented by the vector $\\theta = [\\theta_0, \\theta_1]$ (i.e. the slope and intercept).\n",
      "\n",
      "Another model might consist of the statement \"the observations come from a quadratic curve, with known normal measurement errors\".\n",
      "Mathematically, this says:\n",
      "\n",
      "$$\n",
      "y_{M2}(x;\\theta) = \\theta_0 + \\theta_1 x + \\theta_2 x^2\\\\\n",
      "y \\sim \\mathcal{N}(y_{M2}, \\sigma_y)\n",
      "$$\n",
      "\n",
      "There are three tunable parameters here, again represented by the vector $\\theta$.\n",
      "\n",
      "**Model fitting**, in this case, is the process of finding constraints on the values of the parameters $\\theta$ within each model.\n",
      "That is, it allows you to say, \"given a linear model, this is the best-fit line\" or \"given a quadratic model, this is the best-fit curve.\"\n",
      "\n",
      "**Model selection**, on the other hand, is not concerned with the parameters themselves, but with the question of which model as a whole best describes the data.\n",
      "That is, it allows you go say, \"given my data, a line is a better fit than a quadratic curve\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Concrete Example: Linear or Quadratic?\n",
      "\n",
      "Discussing statistical concepts in the abstract is rarely helpful, and so we'll start with some data about which we can ask concrete modeling questions.\n",
      "\n",
      "Consider the following data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def generate_data(N, rseed=1):\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    x = rng.rand(N)\n",
      "    dy = 0.1 * np.ones(N)\n",
      "    y = x - 0.2 + dy * rng.randn(N)\n",
      "    return np.vstack([x, y, dy]).round(2).T\n",
      "    \n",
      "data = generate_data(20)\n",
      "x, y, sigma_y = data.T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get an idea of what we're looking at, let's visualize it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns; sns.set()\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
      "ax.set(xlabel='x', ylabel='y', title='data');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our question will be this: **what better fits this data: a linear or quadratic curve**?\n",
      "\n",
      "Let's create a function to compute these models given some data and parameters; for convenience, we'll make a very general polynomial model function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def polynomial_fit(theta, x):\n",
      "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
      "    return sum(t * x ** n for (n, t) in enumerate(theta))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As detailed in my previous posts, both the frequentist and Bayesian approaches to model fitting often revolve around the *likelihood*, and a standard frequentist method to determine the best-fit within each model is maximization of this likelihood.\n",
      "Here is a function which computes the log-likelihood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "\n",
      "def logL(theta, model=polynomial_fit, data=data):\n",
      "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
      "    x, y, sigma_y = data.T\n",
      "    y_fit = model(theta, x)\n",
      "    return sum(stats.norm.logpdf(*args)\n",
      "               for args in zip(y, y_fit, sigma_y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Though there are efficient closed-form ways of maximizing this, we'll use a direct optimization approach here for clarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import optimize\n",
      "\n",
      "neg_logL = lambda *args: -logL(*args)\n",
      "theta1 = optimize.fmin(neg_logL, [0, 0], disp=False)\n",
      "theta2 = optimize.fmin(neg_logL, [0, 0, 0], disp=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's now we can visually compare the degree-1 and degree-2 models:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xfit = np.linspace(0, 1, 1000)\n",
      "fig, ax = plt.subplots()\n",
      "ax.errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
      "ax.plot(xfit, polynomial_fit(theta1, xfit), label='best linear model')\n",
      "ax.plot(xfit, polynomial_fit(theta2, xfit), label='best quadratic model')\n",
      "ax.legend(loc='best', fontsize=14)\n",
      "ax.set(xlabel='x', ylabel='y', title='data');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The crux of the question is this: **how can we quantitatively decide which model is a better fit to the data?**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Big Picture: Frequentist vs. Bayesian Model Selection\n",
      "\n",
      "Recall that the fundamental difference between frequentists and Bayesians is a **definition of probability**.\n",
      "\n",
      "Frequentists consider **probabilities as frequencies**: that is, a probability is only meaningful in the context of repeated experiments (even if those repetitions are merely hypothetical).\n",
      "This means, for example, that in the frequentist approach:\n",
      "\n",
      "- *observed data* (and any quantities derived from them) are considered to be random variables: if you make the observations again under similar circumstances, the data may be different, and the details depend on the generating distribution.\n",
      "- *model parameters* (those things that help define the generating distribution) are considered fixed: they aren't subject to a distribution; they just *are*.\n",
      "\n",
      "On the other hand, Bayesians consider **probabilities as degrees-of-belief**: that is, a probability is a way of quantifying our certainty about a particular statement.\n",
      "This means, for example, that in the Bayesian approach:\n",
      "\n",
      "- *observed data* are not directly considered as random variables; they just *are*.\n",
      "- *model parameters* are uncertain quantities and thus subject to probabilistic description.\n",
      "\n",
      "This difference in philosophy has real practical consequences, as shown in my previous posts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notation\n",
      "\n",
      "A quick note on notation. As we are dealing with statements of probability, it is important to be able to denote probabilities concisely.\n",
      "We'll generally be writing conditional probabilities of the form $P(A~|~B)$, which can be read \"the probability of A given B\".\n",
      "Additionally, I'll be using the following shorthands:\n",
      "\n",
      "- $D$ represents observed data\n",
      "- $M$, $M_1$, $M_2$, etc. represents a model\n",
      "- $\\theta$ represents a set of model parameters\n",
      "\n",
      "With this in mind, we'll be writing statements like $P(D~|~\\theta,M)$, which should be read \"the probability of seeing the data, given the parameters $\\theta$ with model $M$.\n",
      "I'm playing a bit fast-and-loose with boolean vs continuous probabilities, but the meaning should be clear from the context."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model Fitting\n",
      "\n",
      "As discussed in previous posts, this difference in the definition of probability, and resulting difference in what quantities are subject to probabilistic description, has actual practical consequences.\n",
      "In particular, when evaluating model parameters, the frequentist and Bayesian approaches deal with different quantities:\n",
      "\n",
      "- frequentists look at the *likelihood*: $P(D~|~\\theta, M)$\n",
      "- Bayesians look at the *posterior*: $P(\\theta~|~D, M)$\n",
      "\n",
      "Here $D$ represents observed data, $M$ represents the model, and $\\theta$ represents the parameters of the model.\n",
      "Note the main distinction: frequentists operate on a **probability of the data**, while Bayesians operate on a **probability of the model**, in line with their respective considerations about the applicability of probability.\n",
      "\n",
      "\n",
      "#### Frequentist Approach\n",
      "\n",
      "Frequentists have an advantage here: the likelihood is something we can compute directly from the model: after all, a model is nothing more than a formula to compute the likelihood.\n",
      "\n",
      "#### Bayesian Approach\n",
      "\n",
      "Bayesians have a more difficult task, but can make progress using Bayes' theorem:\n",
      "\n",
      "$$\n",
      "P(\\theta~|~D,M) = \\frac{P(D~|~\\theta,M) \\cdot P(\\theta~|~M)}{P(D~|~M)} \\propto P(D~|~\\theta,M) \\cdot P(\\theta~|~M)\n",
      "$$\n",
      "\n",
      "Thus we see that the posterior is proportional to the likelihood, and the constant of proportionality involves the prior $P(\\theta~|~M)$, and the evidence $P(D~|~M)$, which in this context can be ignored as a normalization term.\n",
      "For more discussion of model fitting in the frequentist and Bayesian contexts, see the previous posts linked above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model Selection\n",
      "\n",
      "Similarly, when comparing two models $M_1$ and $M_2$, the frequentist and Bayesian approaches look at different quantities:\n",
      "\n",
      "- frequentists compare $P(D~|~M_1)$ and $P(D~|~M_2)$\n",
      "- Bayesians compare $P(M_1~|~D)$ and $P(M_2~|~D)$\n",
      "\n",
      "Notice here that the parameter values no longer appear; this is an important point.\n",
      "We're not out to compare how well *particular fits* of the two models describe data; we're out to compare how well *the models themselves* describe the data.\n",
      "Note that unlike the likelihood, neither probability is directly computable from the model and data, and so we must figure out how to re-express the desired quantity in terms we can compute."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Bayesian Approach\n",
      "Here I would assert that Bayesians have the advantage.\n",
      "Through a combination of Bayes Theorem and probability axioms, you can re-express $P(M~|~D)$ in terms of computable quantities and priors:\n",
      "\n",
      "First, using Bayes' Theorem:\n",
      "\n",
      "$$\n",
      "P(M~|~D) = P(D~|~M)\\frac{P(M)}{P(D)}\n",
      "$$\n",
      "\n",
      "Using the definition of conditional probability, the first term can be expressed as an integral over the parameter space of the likelihood:\n",
      "\n",
      "$$\n",
      "P(D~|~M) = \\int_\\Omega P(D~|~\\theta, M) P(\\theta~|~M) d\\theta\n",
      "$$\n",
      "\n",
      "The remaining terms are priors, but by computing the *odds ratio* of two models, we can cancel-out the most problematic of these:\n",
      "\n",
      "$$\n",
      "O_{21} = \\frac{P(M_2~|~D)}{P(M_1~|~D)} = \\frac{P(D~|~M_2)}{P(D~|~M_1)}\\frac{P(M_2)}{P(M_1)}\n",
      "$$\n",
      "\n",
      "We now have a means of comparing two models via computable quantities: an integral over the likelihood for each, and a prior odds for each.\n",
      "Often the prior odds is assumed to be near unity, leaving only the well-defined (but often computationally intensive) integral over likelihood for each model.\n",
      "More on this below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Frequentist Approach\n",
      "\n",
      "For model selection, frequentists are working with the quantity $P(D~|~M)$.\n",
      "Notice that unlike Bayesians, they *cannot* express this as an integral over parameter space, because the notion of a probability distribution over model parameters does not make sense in the frequentist context.\n",
      "But recall that frequentists can make probabilistic statements about data or quantities derived from them: with this in mind, we can make progress by computing some *statistic* from the data for which the distribution is known.\n",
      "The difficulty is that which statistic is most useful depends highly on the precise model and data being used, and so practicing frequentist statistics requires a breadth of background knowledge about the assumptions made by various approaches.\n",
      "\n",
      "For example, one commonly-seen distribution for data-derived statistis is the [$\\chi^2$ (chi-squared) distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution).\n",
      "The $\\chi^2$ distribution with $K$ degrees of freedom describes the distribution of a sum of squares of $K$ independent normally-distributed variables.\n",
      "We can use Python tools to quickly visualize this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import stats\n",
      "v = np.linspace(0, 8, 1000)\n",
      "for k in range(1, 7):\n",
      "    plt.plot(v, stats.chi2.pdf(v, k),\n",
      "             label=\"k = {0}\".format(k))\n",
      "plt.legend(ncol=2)\n",
      "plt.gca().set(title='$\\chi^2$ distribution', xlabel='x', ylabel='p(x)', ylim=(0, 0.5));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The key observation is this: if we can somehow *transform our data* into a single statistic $S(D;\\theta,M)$ which we expect behaves like the sum of squares of normally-distributed values, then we can analyze the likelihood of this statistic in terms of this $\\chi^2$ distribution.\n",
      "\n",
      "In the case of our linear and quadratic model likelihoods, the models are built on the explicit expectation that for the correct model, the observed data $y$ will be normally distributed about the model value $y_M$ with a standard deviation $\\sigma_y$.\n",
      "Thus the sum of squares of error-normalized residuals should follow the $\\chi^2$ distribution, and so we construct our $\\chi^2$ statistic:\n",
      "\n",
      "$$\n",
      "\\chi^2(D;\\theta,M) = \\sum_n\\left[\\frac{y_n - y_M(x_n;\\theta)}{\\sigma_y}\\right]^2\n",
      "$$\n",
      "\n",
      "Coincidentally (or is it?), this statistic is proportional to the negative log likelihood, up to a constant offset.\n",
      "\n",
      "Notice what we've done here: we've replaced an *uncomputable* quantity $P(D~|~M)$ with a *computable* quantity $P(S~|~M_S)$, where $S = \\chi^2$ is a particular transformation of the data for which (under our model assumptions) we can compute the probability distribution.\n",
      "So rather than asking \"how likely are we to see the data $D$ under the model $M$\", we can ask \"how likely are we to see the statistic $S$ under the model $M_S$\", and the two likelihoods are essentially equivalent *as long as our assumptions hold*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Theory to Practice: Linear or Quadratic?\n",
      "\n",
      "Let's use the ideas developed above to address the above model selection problem in both a frequentist and Bayesian context.\n",
      "First, though, let's take a look at what **not** to do."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Can We Directly Compare Likelihoods?\n",
      "\n",
      "One common mistake is to assume that we can select between models via *the value of the maximum likelihood*.\n",
      "While this works in some cases, it is not generally applicable.\n",
      "Let's take a look at the maximum log-likelihood value for each of our fits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"linear model:    logL =\", logL(theta1))\n",
      "print(\"quadratic model: logL =\", logL(theta2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The quadratic model yields a higher log-likelihood, but this **does not** necessarily mean it is the better model!\n",
      "\n",
      "The problem is that the quadratic model has more degrees of freedom than the linear model, and thus will have an equal or larger maximum likelihood for **any data**.\n",
      "This also extends to higher-order polynomial models: using this maximum likelihood comparison, a cubic model will always outperform a quadratic model, a quartic model will outperform a cubic, etc.\n",
      "Let's take a look at this by plotting the maximum log-likelihood as a function of polynomial degree:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def best_theta(degree):\n",
      "    theta_0 = (degree + 1) * [0]\n",
      "    return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)\n",
      "    \n",
      "degrees = np.arange(1, 10)\n",
      "thetas = [best_theta(d) for d in degrees]\n",
      "logL_max = [logL(theta) for theta in thetas]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
      "ax[0].plot(degrees, logL_max)\n",
      "ax[0].set(xlabel='degree', ylabel='log(Lmax)')\n",
      "\n",
      "ax[1].errorbar(x, y, sigma_y, fmt='ok', ecolor='gray')\n",
      "ylim = ax[1].get_ylim()\n",
      "for (degree, theta) in zip(degrees, thetas):\n",
      "    if degree not in [1, 2, 9]: continue\n",
      "    ax[1].plot(xfit, polynomial_fit(theta, xfit),\n",
      "               label='degree={0}'.format(degree))\n",
      "ax[1].set(ylim=ylim, xlabel='x', ylabel='y')\n",
      "ax[1].legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected, we see in the left panel that the maximum likelihood *value* increases as we increase the degree of the polynomial, but looking at the right panel, we would not say that a ninth-order polynomial is a *better model*.\n",
      "The more complicated models achieve a high maximum likelihood by **over-fitting** the data, responding to the individual noise in the points rather than the underlying trend."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Frequentist Model Selection: $\\chi^2$\n",
      "\n",
      "Let's go beyond likelihood comparisons, and take a look at what the $\\chi^2$ statistic says about these models.\n",
      "Recall that we expect for the true model, the sum of normalized residuals will be drawn from a $\\chi^2$ distribuion with a certain number of degrees of freedom related to the number of data points.\n",
      "For this linear model, the degrees of freedom are $dof = N - K$, where $N$ is the number of data points and $K$ is the number of model parameters.\n",
      "\n",
      "Let's make functions to compute the $\\chi^2$ and the number of degrees of freedom, and evaluate the likelihood of each model with the $\\chi^2$ distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_chi2(degree, data=data):\n",
      "    x, y, sigma_y = data.T\n",
      "    theta = best_theta(degree)\n",
      "    resid = (y - polynomial_fit(theta, x)) / sigma_y\n",
      "    return np.sum(resid ** 2)\n",
      "\n",
      "def compute_dof(degree, data=data):\n",
      "    return data.shape[0] - (degree + 1)\n",
      "\n",
      "def chi2_likelihood(degree, data=data):\n",
      "    chi2 = compute_chi2(degree, data)\n",
      "    dof = compute_dof(degree, data)\n",
      "    return stats.chi2(dof).pdf(chi2)\n",
      "\n",
      "print(\"linear model:    logL =\", chi2_likelihood(1))\n",
      "print(\"quadratic model: logL =\", chi2_likelihood(2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the $\\chi^2$ model, we see that the linear model is marginally more likely, even though the maximum likelihood of the model is smaller!\n",
      "\n",
      "It might help to visualize this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(2)\n",
      "for axi, degree in zip(ax, [1, 2]):\n",
      "    v = np.linspace(0, 40, 1000)\n",
      "    chi2_dist = stats.chi2(compute_dof(degree)).pdf(v)\n",
      "    chi2_val = compute_chi2(degree)\n",
      "    axi.fill(v, chi2_dist, alpha=0.3)\n",
      "    axi.axvline(chi2_val)\n",
      "    axi.set_title('degree = {0}'.format(degree))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see visually here how this procedure corrects for model complexity: even though the $\\chi^2$ *value* for the quadratic model is lower, the differing degrees of freedom change the *likelihood* of seeing that value."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Significance of the Comparison\n",
      "\n",
      "But how much should we trust this conclusion?\n",
      "Just as before, we can address this by constructing a statistic which compares the two results, for which we have a known distribution.\n",
      "Luckily, the difference of $\\chi^2$ statistics here *also follows a $\\chi^2$ distribution*, with 1 degree of freedom. This is due to the fact that the models are *nested* \u2013 that is, the linear model is a specialization of the quadratic model (for some background, look up the [Likelihood Ratio Test](https://en.wikipedia.org/wiki/Likelihood-ratio_test)).\n",
      "Essentially, we are treating the linear model as the *null hypothesis*, and asking if there is sufficient evidence to justify the quadratic model.\n",
      "Let's plot this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chi2_diff = compute_chi2(1) - compute_chi2(2)\n",
      "\n",
      "v = np.linspace(0, 5, 1000)\n",
      "chi2_dist = stats.chi2(1).pdf(v)\n",
      "p_value = 1 - stats.chi2(1).cdf(chi2_diff)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "ax.fill_between(v, 0, chi2_dist, alpha=0.3)\n",
      "ax.fill_between(v, 0, chi2_dist * (v > chi2_diff), alpha=0.5)\n",
      "ax.axvline(chi2_diff)\n",
      "ax.set(ylim=(0, 1), xlabel=\"$\\chi^2$ difference\", ylabel=\"probability\");\n",
      "ax.text(4.9, 0.95, \"p = {0:.2f}\".format(p_value),\n",
      "        ha='right', va='top', size=14);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we see where this $\\chi^2$ difference lies on its expected distribution, under the null hypothesis that the linear model is the true model.\n",
      "The area of the distribution larger than this observed value is known as the [*p value*](https://en.wikipedia.org/wiki/P-value): the probability that the test would favor the more quadratic distribution by chance.\n",
      "Here we see that there is a 17% probability that our results would favor the quadratic model by chance.\n",
      "We should interpret this to say that the results are *not inconsistent with the null hypothesis*: that is, our data favors the linear model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Other Frequentist Approaches\n",
      "\n",
      "- AIC\n",
      "- Cross-Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Frequentist Model Selection Approaches\n",
      "\n",
      "As detailed in my previous posts, the central philosophical difference between the frequentist and Bayesian views is that frequentists consider model parameters fixed and data subject to probability, while Bayesians consider data fixed and model parameters subject to probability.\n",
      "Thus frequentist model selection mostly derives from attempts to quantify the distribution of a data-derived statistic.\n",
      "Though this list is by no means complete, some of the most popular frequentist model selection approaches are the **reduced chi-square**, the **Aikake Information Criterion (AIC)**, and **Cross-validation**.\n",
      "We'll briefly demonstrate these here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Reduced chi-square\n",
      "\n",
      "The chi-square $(\\chi^2)$ statistic of a model is proportional to the exponent of the likelihood used above.\n",
      "For a model $y_M$, the value is:\n",
      "\n",
      "$$\n",
      "\\chi^2 = \\sum_n \\frac{(y_{M, n} - y_n)^2}{\\sigma_y^2}\n",
      "$$\n",
      "\n",
      "The name of this statistic comes from the [Chi-square distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution), because under the assumption that residuals are independent and normally distributed, the statistic will follow some form of this distribution.\n",
      "Under these assumptions, if you find a $\\chi^2$ which is highly improbable according to this distribution, it is a sign that your model does not fit the data well.\n",
      "\n",
      "The expectation value of a $\\chi^2$ distribution is the number of degrees of freedom in the fit.\n",
      "For $N$ points and $K$ model parameters, this can often be approximated by $DOF = N - K$ (though see some discussion in [this very interesting paper](http://arxiv.org/abs/1012.3754)).\n",
      "From this comes the concept of \"reduced chi-square\", or \"chi-square per degree of freedom\":\n",
      "\n",
      "$$\n",
      "\\chi^2_{red} = \\frac{\\chi^2}{N - K}\n",
      "$$\n",
      "\n",
      "If the assumptions about the model hold, $\\chi^2_{red}$ should be drawn from a distribution centered at 1 with a variance of $v = 2 / (N - K)$.\n",
      "So what does the reduced chi-square test say about our polynomial models? Let's create functions to compute the chi-square and reduced chi-square statistics:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chi2(theta, model=polynomial_fit, data=data):\n",
      "    \"\"\"chi-square statistic for model\"\"\"\n",
      "    x, y, dy = data.T\n",
      "    y_fit = model(theta, x)\n",
      "    return sum((y - y_fit) ** 2 / dy ** 2)\n",
      "\n",
      "def chi2_reduced(theta, model=polynomial_fit, data=data):\n",
      "    \"\"\"reduced chi-square statistic for model\"\"\"\n",
      "    dof = data.shape[0] - len(theta)\n",
      "    return chi2(theta, model, data) / dof"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can compute these statistics for a range of model degrees:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "degree = np.arange(1, 15)\n",
      "models = [PolynomialRegression(d).fit(x[:, None], y)\n",
      "          for d in degree]\n",
      "chi2_max = np.array([chi2(model.get_theta()) for model in models])\n",
      "chi2_reduced_max = np.array([chi2_reduced(model.get_theta()) for model in models])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally we will plot these together:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = data.shape[0]\n",
      "\n",
      "plt.plot(degree, chi2_max / N, label='chi-square (scaled for comparison)')\n",
      "plt.plot(degree, chi2_reduced_max, label='reduced chi-square')\n",
      "plt.xlabel('polynomial degree')\n",
      "plt.ylabel('$\\chi^2$')\n",
      "plt.legend();"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to this test, the model with reduced chi-square closest to its expected value of one is likely to be the \"correct\" model.\n",
      "With this in mind, we see that the reduced chi-square test strongly prefers a degree-2 model over a degree-1 model, and marginally prefers a degree-2 model over a higher-degree model.\n",
      "\n",
      "(Notice also that our reduced chi-square curve is near 1 at a very high polynomial degree: I believe this is due to numerical inaccuracies in the fit as you move to high polynomial degree, which artificially inflate the $\\chi^2$).\n",
      "\n",
      "Keep in mind that this entire test relies on the assumption that model residuals are independent and normally distributed, which may or may not be a correct assumption!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Aikake Information Criterion\n",
      "\n",
      "Another frequentist approach to model selection is to add a correction to the maximum likelihood which penalizes overly-complicated models.\n",
      "One particularly simple approach to this is to use an *information criterion*, such as the [Aikiake Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion), derived from considerations of information loss, under the assumption that the model is univariate, linear, has normally-distributed residuals, and that $N$ is very large.\n",
      "The simple formula is this, where $K$ is the number of model parameters:\n",
      "\n",
      "$$\n",
      "AIC = -2\\log L + 2K\n",
      "$$\n",
      "\n",
      "The model which minimizes the AIC is the best model according to this criterion.\n",
      "Let's compare the log-likelihood and the AIC for our polynomial model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def AIC(theta, model=polynomial_fit, data=data):\n",
      "    \"\"\"chi-square statistic for model\"\"\"\n",
      "    return -2 * logL(theta, model, data) + 2 * len(theta)\n",
      "\n",
      "degree = np.arange(1, 15)\n",
      "models = [PolynomialRegression(d).fit(x[:, None], y)\n",
      "          for d in degree]\n",
      "AIC_ = np.array([AIC(model.get_theta()) for model in models])\n",
      "logL_ = [-2 * logL(model.get_theta()) for model in models]\n",
      "plt.plot(degree, AIC_, label='AIC')\n",
      "plt.plot(degree, logL_, label='-2 logL')\n",
      "plt.xlabel('polynomial degree')\n",
      "plt.legend(loc='best', fontsize=14);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, here the best model should *minimize the AIC* if the assumptions hold.\n",
      "We can see here how the AIC penalizes more complicated models; the AIC seems to favor a degree-2 or degree-3 model in this case over any others.\n",
      "\n",
      "Under the assumptions of AIC, the relative likelihood of two models is the exponent of the difference:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.exp(AIC_[degree == 3] - AIC_[degree == 2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So here, we see that the degree 2 model is 0.7 times as probable as the degree 3 model to minimize the information loss."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross-Validation\n",
      "\n",
      "The reduced chi-square and the AIC both depend on strong assumptions about the model and the data; a less rigid (though more computationally intensive) means of assessing model fit is to use *Cross-validation*.\n",
      "Effectively, this involves **fitting the model on one subset of data**, while **evaluating the model on another** disjoint set.\n",
      "By doing this procedure, models which over-fit the noise in the data will automatically be penalized.\n",
      "\n",
      "Cross-validation is useful enough that it is built-in to scikit-learn; if you use a scikit-learn-style estimator like we did above, the implementation is simple.\n",
      "Here we'll use a [K-fold cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold) on our polynomial model to determine the best model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score, LeaveOneOut\n",
      "\n",
      "def CV_score(degree):\n",
      "    cv = cross_val_score(PolynomialRegression(degree), x[:, None], y)\n",
      "    return cv.mean()\n",
      "\n",
      "degree = np.arange(1, 8)\n",
      "CV = [CV_score(d) for d in degree]\n",
      "plt.plot(degree, CV)\n",
      "plt.xlabel('polynomial degree')\n",
      "plt.ylabel('negative MSE');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default the score for a linear regression in scikit-learn is a [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), and a larger value indicates a better model.\n",
      "We see that according to the cross-validation procedure, degree-3 and degree-4 models are favored compared to simpler and more complex models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bayesian Model Selection\n",
      "\n",
      "Recall that in the Bayesian context, we can meaningfully talk about the probability of model parameters.\n",
      "\n",
      "- Odds Ratio http://stronginference.com/bayes-factors-pymc.html\n",
      "\n",
      "- Harmonic Mean: https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Fitting the Models\n",
      "\n",
      "We'll use [emcee](http://dan.iel.fm/emcee/)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_prior(theta):\n",
      "    if np.any(abs(theta) > 100):\n",
      "        return -np.inf  # log(0)\n",
      "    else:\n",
      "        return 200 ** -len(theta)\n",
      "    \n",
      "def y_model(theta, x):\n",
      "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
      "\n",
      "def log_likelihood(theta, x, y, dy):\n",
      "    yM = y_model(theta, x)\n",
      "    return -0.5 * np.sum(np.log(2 * np.pi * dy ** 2) + (y - yM) ** 2 / dy ** 2)\n",
      "\n",
      "def log_posterior(theta, x, y, dy):\n",
      "    theta = np.asarray(theta)\n",
      "    return log_prior(theta) + log_likelihood(theta, x, y, dy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import emcee\n",
      "\n",
      "def polymodel_mcmc(degree,\n",
      "                   x=x, y=y, dy=dy,\n",
      "                   log_posterior=log_posterior,\n",
      "                   nwalkers=50, nburn=1000, nsteps=2000):\n",
      "    ndim = degree + 1\n",
      "    rng = np.random.RandomState(0)\n",
      "    starting_guesses = rng.randn(nwalkers, ndim)\n",
      "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x, y, dy])\n",
      "    sampler.run_mcmc(starting_guesses, nsteps)\n",
      "    trace = sampler.chain[:, nburn:, :].reshape(-1, ndim)\n",
      "    return trace"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace1 = polymodel_mcmc(1)\n",
      "data1 = pd.DataFrame(trace1, columns=['intercept', 'slope'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with sns.axes_style('ticks'):\n",
      "    jnt = sns.jointplot('intercept', 'slope', data=data1, kind=\"hex\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace2 = polymodel_mcmc(degree=2)\n",
      "data2 = pd.DataFrame(trace2, columns=[r'$\\theta_0$ (intercept)',\n",
      "                                      r'$\\theta_1$ (slope)',\n",
      "                                      r'$\\theta_2$ (quadratic)'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the colormap from the joint plot above\n",
      "cmap = jnt.ax_joint.collections[0].get_cmap()\n",
      "\n",
      "def hexbin(x, y, color, cmap=cmap, **kwargs):\n",
      "    plt.hexbin(x, y, gridsize=50, cmap=cmap, **kwargs)\n",
      "\n",
      "with sns.axes_style('ticks'):\n",
      "    grid = sns.PairGrid(data2)\n",
      "    grid.map_diag(plt.hist, bins=30, alpha=0.5)\n",
      "    grid.map_offdiag(hexbin);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we compute the Bayes factor...\n",
      "\n",
      "(show math)\n",
      "\n",
      "Integrate the result with scipy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import integrate\n",
      "\n",
      "def integrate_posterior(trace, log_posterior, x=x, y=y, dy=dy):\n",
      "    tmin = trace.min(0)\n",
      "    tmax = trace.max(0)\n",
      "    if trace.shape[1] == 1:\n",
      "        func = lambda theta0: log_posterior([theta0], x, y, dy)\n",
      "        Z, dZ = integrate.quad(func, tmin[0], tmax[0])\n",
      "    elif trace.shape[1] == 2:\n",
      "        func = lambda theta1, theta0: np.exp(log_posterior([theta0, theta1], x, y, dy))\n",
      "        Z, dZ = integrate.dblquad(func,\n",
      "                                  tmin[0], tmax[0],\n",
      "                                  lambda x: tmin[1], lambda x: tmax[1])\n",
      "    elif trace.shape[1] == 3:\n",
      "        func = lambda theta2, theta1, theta0: np.exp(log_posterior([theta0, theta1, theta2], x, y, dy))\n",
      "        Z, dZ = integrate.tplquad(func,\n",
      "                                  tmin[0], tmax[0],\n",
      "                                  lambda x: tmin[1], lambda x: tmax[1],\n",
      "                                  lambda x, y: tmin[2], lambda x, y: tmax[2])\n",
      "    else:\n",
      "        raise ValueError(\"only supports 1, 2, or 3 dimensional data\")\n",
      "    \n",
      "    return Z, dZ"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z1, dZ1 = integrate_posterior(trace1, log_posterior)\n",
      "print(\"Z_1 = {0:.3g} +/- {1:.2g}\".format(Z1, dZ1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z2, dZ2 = integrate_posterior(trace2, log_posterior)\n",
      "print(\"Z_2 = {0:.3g} +/- {1:.2g}\".format(Z2, dZ2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Odds Ratio: {0:.0f}\".format(Z2 / Z1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the Bayesian approach favors a degree-2 model over a degree-1 model by odds of over 5000 to 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### degree 1 model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace1 = polymodel_mcmc(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z1_direct = compute_Z_direct(trace1, log_posterior)\n",
      "Z1_KDE = compute_Z_KDE(trace1, log_posterior)\n",
      "Z1_HM = compute_Z_HM(trace1, log_likelihood)\n",
      "\n",
      "print(\"direct:\", Z1_direct)\n",
      "print(\"KDE:   \", Z1_KDE)\n",
      "print(\"HM:    \", Z1_HM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### degree-2 model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trace2 = polymodel_mcmc(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z2_direct = compute_Z_direct(trace2, log_posterior)\n",
      "Z2_KDE = compute_Z_KDE(trace2, log_posterior)\n",
      "Z2_HM = compute_Z_HM(trace2, log_likelihood)\n",
      "\n",
      "print(\"direct:\", Z2_direct)\n",
      "print(\"KDE:   \", Z2_KDE)\n",
      "print(\"HM:    \", Z2_HM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### BIC\n",
      "\n",
      "$$\n",
      "BIC = -2\\log L + k\\log N\n",
      "$$\n",
      "\n",
      "$$\n",
      "AIC = -2\\log L + 2k\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_BIC(degree, x, y, dy):\n",
      "    return (degree + 1) * np.log(len(x)) - 2 * compute_logL(degree, x, y, dy)\n",
      "\n",
      "AIC_results = [compute_AIC(degree, x, y, dy)\n",
      "               for degree in degrees]\n",
      "BIC_results = [compute_BIC(degree, x, y, dy)\n",
      "               for degree in degrees]\n",
      "\n",
      "plt.plot(degrees, BIC_results,\n",
      "         label='BIC')\n",
      "plt.plot(degrees, AIC_results,\n",
      "         label='AIC')\n",
      "plt.xlabel('degree')\n",
      "plt.legend(loc='best');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Other options\n",
      "\n",
      "- Harmonic mean method: it's common, but horrible. You basically should [never use it](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/). However, some have proposed [modified versions](http://projecteuclid.org/euclid.ba/1346158782). PyMC doesn't currently implement this, but for models of constant dimension you can do something [very similar](http://stronginference.com/bayes-factors-pymc.html))\n",
      "- Estimation by thermodynamic tempering (e.g. in [Parallel Tempering Ensemble MCMC](http://dan.iel.fm/emcee/current/user/pt/))\n",
      "- [Nested Sampling](https://en.wikipedia.org/wiki/Nested_sampling_algorithm) or [Reversible Jump MCMC](https://en.wikipedia.org/wiki/Reversible-jump_Markov_chain_Monte_Carlo)\n",
      "- Posterior Predictive Checks (a good resource is [this 70-page review of Bayesian model selection](http://projecteuclid.org/download/pdfview_1/euclid.ssu/1356628931)\n",
      "- http://drsmorey.org/bibtex/upload/Morey:etal:2011a.pdf\n",
      "- Kernel Density Estimation \u2013 I can't find any references, but the idea seems obvious (what's wrong with it?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_Z_HM(trace, log_likelihood, x=x, y=y, dy=dy):\n",
      "    logL = np.array([log_likelihood(theta, x, y, dy) for theta in trace])\n",
      "    return len(logL) / np.sum(np.exp(-logL))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KernelDensity\n",
      "\n",
      "def compute_Z_KDE(trace, log_posterior, x=x, y=y, dy=dy):\n",
      "    h = 0.01 * np.mean(trace.max(0) - trace.min(0))\n",
      "    kde = KernelDensity(h, rtol=1E-3)\n",
      "    kde.fit(trace1[::10])\n",
      "    logp = kde.score_samples(trace1[::10])\n",
      "    logL = np.array([log_posterior(theta, x, y, dy)\n",
      "                     for theta in trace[::10]])\n",
      "    log_Z2_estimate = logL - logp\n",
      "    return np.exp(np.median(log_Z2_estimate))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_data(N, rseed=42):\n",
      "    rng = np.random.RandomState(rseed)\n",
      "    x = rng.rand(N)\n",
      "    dy = 0.2 * np.ones(N)\n",
      "    y = 2 * x ** 2 + 0.5 * x + dy * rng.randn(N)\n",
      "    xfit = np.linspace(0, 1)\n",
      "    return np.vstack([x, y, dy]).round(3).T\n",
      "    \n",
      "data = generate_data(20)\n",
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}